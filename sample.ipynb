{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out-stock' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 304.11M\n",
      "Loading meta from data/stock/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is tensor([[[1.3750, 1.9512, 1.7324,  ..., 1.7227, 1.0674, 1.7080],\n",
      "         [1.5439, 1.0820, 1.4580,  ..., 1.9287, 1.3320, 1.4658],\n",
      "         [1.0137, 1.0820, 1.2588,  ..., 1.3613, 1.1084, 1.8760],\n",
      "         ...,\n",
      "         [1.2646, 1.0977, 1.5430,  ..., 1.5859, 1.7656, 1.8799],\n",
      "         [1.8291, 1.1279, 1.7441,  ..., 1.4326, 1.8789, 1.4580],\n",
      "         [1.3428, 1.6299, 1.6191,  ..., 1.8438, 1.6211, 1.6465]]],\n",
      "       dtype=torch.float16)\n",
      "X is tensor([[[1.3750, 1.9512, 1.7324,  ..., 1.7227, 1.0674, 1.7080],\n",
      "         [1.5439, 1.0820, 1.4580,  ..., 1.9287, 1.3320, 1.4658],\n",
      "         [1.0137, 1.0820, 1.2588,  ..., 1.3613, 1.1084, 1.8760],\n",
      "         ...,\n",
      "         [1.8516, 1.4912, 1.2881,  ..., 1.7627, 1.6074, 1.6445],\n",
      "         [1.2646, 1.0977, 1.5430,  ..., 1.5859, 1.7656, 1.8799],\n",
      "         [1.8291, 1.1279, 1.7441,  ..., 1.4326, 1.8789, 1.4580]]],\n",
      "       dtype=torch.float16), X.shape is torch.Size([1, 32, 2048])\n",
      "Y is tensor([[1.3428, 1.6299, 1.6191,  ..., 1.8438, 1.6211, 1.6465]],\n",
      "       dtype=torch.float16), Y.shape is torch.Size([1, 2048])\n",
      "value is tensor([[2.0000, 1.9990, 1.9980, 1.9980, 1.9971, 1.9971, 1.9961, 1.9961, 1.9951,\n",
      "         1.9951]], dtype=torch.float16)\n",
      "index is tensor([[ 968,  222,  166,  460, 1292, 1651, 1970, 1104, 1295, 1306]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成50x32大小的数组，值范围在1到10之间\n",
    "data = np.random.rand(33, 2048) + 1\n",
    "\n",
    "# 将NumPy数组转换为pandas DataFrame\n",
    "data = pd.DataFrame(data)\n",
    "data = torch.tensor(data.values, dtype=torch.float16)\n",
    "data = data.unsqueeze(0)\n",
    "print(f'data is {data}')\n",
    "\n",
    "eval_X = data[:, :32, :]\n",
    "eval_Y = data[:, -1, :]\n",
    "\n",
    "\n",
    "print(f'X is {eval_X}, X.shape is {eval_X.shape}')\n",
    "print(f'Y is {eval_Y}, Y.shape is {eval_Y.shape}')\n",
    "value, index = torch.topk(eval_Y, 10, -1)\n",
    "print(f'value is {value}')\n",
    "print(f'index is {index}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict index is tensor([[ 111,  378,  491,   70,  257,   63,  174,  722,  625,  931,  477,  567,\n",
      "          592,  193,  878,  199,  304,  915,  744,  770,  889,  589,  468,  515,\n",
      "          599,  699,  119,  531,  427,  446,   32,  517,  757,  781,  855,  488,\n",
      "          841, 1014,  998,  605,  702,  634,  650,  284,  615,  681, 1002,   62,\n",
      "          946,  740,  518,  212,  810,  647,  907,  358,  670,  289,  887,  402,\n",
      "          172,  248,  945,  315,  825,  311,   96,  626,  750,  167,  754,  158,\n",
      "          811,  299,  717,    2,   18,  559,  904,  109,  463,  732,  388,  457,\n",
      "          916,  927,  105,  718,  355,  272,    7,  903,  636,  787,  867,  322,\n",
      "           50,  780,   37,  643,  761,  723,  259,   35,  510, 1008,  241,  769,\n",
      "          440,  783,  376,  928,  649,  177,  392,  145,  874,  412,  914,  772,\n",
      "          250,  264,  863,  337,   66,  560,  252,  314,  657,  848,  991,  467,\n",
      "           73,  957,  977,   30,  335,  204,  972,  503,  826,  812,  281,  347,\n",
      "           34,  265,  694,  433,  658,  326, 1000,  679,  738,  674,  130,  909,\n",
      "          461,  857,  365,  590,   94,  899,  556,  712,  846,  849,  363,  290,\n",
      "           31,   39,  383,  469,  665,  198,  324,  617,  123,  214,  447,  693,\n",
      "          941,  908,  759,  706,  710,  728,  879,  968,  565,  275,  197,  184,\n",
      "          866,  104,  703,  302,  989,   38,  924,  173]])\n",
      "predict probs is tensor([[0.0147, 0.0094, 0.0094, 0.0089, 0.0079, 0.0069, 0.0068, 0.0067, 0.0067,\n",
      "         0.0065, 0.0058, 0.0056, 0.0055, 0.0053, 0.0052, 0.0051, 0.0048, 0.0047,\n",
      "         0.0046, 0.0046, 0.0046, 0.0045, 0.0045, 0.0044, 0.0043, 0.0043, 0.0042,\n",
      "         0.0041, 0.0041, 0.0040, 0.0039, 0.0039, 0.0039, 0.0038, 0.0038, 0.0038,\n",
      "         0.0036, 0.0036, 0.0035, 0.0035, 0.0035, 0.0035, 0.0034, 0.0034, 0.0034,\n",
      "         0.0034, 0.0033, 0.0033, 0.0033, 0.0032, 0.0031, 0.0031, 0.0031, 0.0030,\n",
      "         0.0030, 0.0030, 0.0030, 0.0030, 0.0029, 0.0029, 0.0029, 0.0028, 0.0028,\n",
      "         0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0027, 0.0027,\n",
      "         0.0027, 0.0027, 0.0027, 0.0026, 0.0026, 0.0026, 0.0025, 0.0025, 0.0025,\n",
      "         0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0024, 0.0024, 0.0024, 0.0024,\n",
      "         0.0023, 0.0023, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
      "         0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
      "         0.0022, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0020, 0.0020,\n",
      "         0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0019, 0.0019,\n",
      "         0.0019, 0.0019, 0.0019, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
      "         0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
      "         0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
      "         0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0015, 0.0015,\n",
      "         0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
      "         0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014]])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:  \n",
    "        probs, index = model.generate_last(eval_X, top_k=top_k)\n",
    "        print(f'predict index is {index}, index.shape is {index.shape}')\n",
    "        print(f'predict probs is {probs}, probs.shape is {probs.shape}')\n",
    "        print('---------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
