{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out-stock'\n",
    "eval_interval = 100\n",
    "log_interval = 100\n",
    "eval_iters = 2\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'stock'\n",
    "wandb_run_name = 'mini-gpt' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'stock'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 24 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 5\n",
    "# model\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 512 # 参数估计：params= layer*(12*emb**2), 数据量估计dataSize= 10 *params\n",
    "# dropout = 0.998728434 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "dropout = 0\n",
    "# dropout = 1-dataSize/10/params\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "meta_vocab_size = n_embd // 2\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 1e6 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "master_process = True\n",
    "\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datadir = os.path.join('data', dataset)\n",
    "\n",
    "# meta数据\n",
    "meta = {}\n",
    "with open(os.path.join(datadir, 'meta.pkl'), 'r') as f:\n",
    "    meta = json.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    meta_vocab_size = 4096\n",
    "def decode(id):\n",
    "    return meta['itos'][str(id)]\n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "\n",
    "df_data = pd.read_csv(os.path.join(datadir, 'train.csv')).iloc[1:,:meta_vocab_size+1]\n",
    "\n",
    "length = len(df_data)\n",
    "pd_train_data = df_data.iloc[:int(length*0.95)]\n",
    "pd_train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd_val_data = df_data.iloc[int(length*0.95)-block_size:int(length*0.98)]\n",
    "pd_val_data = df_data.iloc[int(length*0.95)-block_size:]\n",
    "pd_val_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd_test_data = df_data.iloc[int(length*0.98)-block_size:]\n",
    "pd_test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "pd_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_frame_to_id(dataframe):\n",
    "    train_data = dataframe.iloc[:, 1:]\n",
    "    # 对所有行，都取前10个最大的\n",
    "    def top_n(row, n):\n",
    "        # return row.nlargest(n).values\n",
    "        return row.nlargest(n).index.tolist()\n",
    "\n",
    "    n = batch_size\n",
    "    data_top_10 = train_data.apply(top_n, axis=1, n=n)\n",
    "\n",
    "    # 将结果转换为 [266, 10] 的形状\n",
    "    data_transformed = pd.DataFrame(data_top_10.tolist(), index=train_data.index)\n",
    "\n",
    "    def to_id(row):\n",
    "        return encode(row)\n",
    "    \n",
    "    data_transformed = data_transformed.apply(to_id, axis=1)\n",
    "    data_transformed = torch.stack([torch.tensor(row) for row in data_transformed])\n",
    "    return data_transformed\n",
    "\n",
    "train_data = trans_frame_to_id(pd_train_data)\n",
    "val_data = trans_frame_to_id(pd_val_data)\n",
    "\n",
    "print(f'train.shape is {train_data.shape}, val.shape is {val_data.shape}')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写个python程序，从pandas数据中获取一批训练数据：\n",
    "# 1. 读取pandas数据A，格式是：[trade_date, label1, label2, label3，。。。]，date的样例有：20230104，label*是数字，样例有1.0399、0.9943\n",
    "# 2. X是生成指定格式的shape = [shape, ]\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def get_batch(split, index=None):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    pd_data = pd_train_data if split == 'train' else pd_val_data\n",
    "    # data = val_data\n",
    "\n",
    "    if index is None:\n",
    "        indices = torch.randint(len(data)-1-block_size, (batch_size, ))\n",
    "        index_block = torch.randint(data.shape[-1], (batch_size, block_size + 1, 1))\n",
    "        # print(index_block)\n",
    "\n",
    "        # (batch, block)\n",
    "        x = torch.stack([data[i:i+block_size] for i in indices])\n",
    "        x = x.gather(2, index_block[:,:block_size,:]).squeeze(-1)\n",
    "    \n",
    "        # (batch, block)\n",
    "        y = torch.stack([data[i+1:i+1+block_size] for i in indices])\n",
    "        y = y.gather(2, index_block[:,1:,:]).squeeze(-1)\n",
    "    else:\n",
    "        index_block = torch.randint(data.shape[-1], (block_size, 1))\n",
    "        x = data[index: index+block_size]\n",
    "        x = x.gather(-1, index_block).squeeze(-1)\n",
    "        x = torch.stack([x for i in range(data.shape[-1])])\n",
    "        \n",
    "        y_predict = data[index+block_size].reshape(x.shape[0], -1)\n",
    "        y = torch.cat((x[:,1:], y_predict), dim=1)\n",
    "\n",
    "    # index_first = index\n",
    "    # if index is None:\n",
    "    #     index_first = indices[0].item()\n",
    "    # for i in range(block_size):\n",
    "    #     print(f'x is date={pd_data.iloc[index_first+i, 0]}, chg={pd_data.loc[index_first+i, decode(x[0][i].item())]:<6}, code={decode(x[0][i].item())}, code_id={x[0][i].item()}')\n",
    "    # print('----')\n",
    "    # for i in range(block_size):\n",
    "    #      print(f'y is date={pd_data.iloc[index_first+i+1, 0]}, chg={pd_data.loc[index_first+i+1, decode(y[0][i].item())]:<6}, code={decode(y[0][i].item())}, code_id={y[0][i].item()}')\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train', 0)\n",
    "print(f'x.shape is {x.shape}, y.shape is {y.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=meta_vocab_size, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            data = train_data if split == 'train' else val_data\n",
    "            X, Y = get_batch(split, random.randint(0, len(data)-block_size-1))\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
    "\n",
    "X, Y = get_batch('train', 1)\n",
    "\n",
    "\n",
    "print(f'X is {X}')\n",
    "print(f'Y is {Y}')\n",
    "logits, loss = model(X, Y)\n",
    "print(logits.shape)\n",
    "# print(f'logits is {logits}, shape is {logits.shape}')\n",
    "# print(f'loss is {loss}, loss.shape is {loss.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# training loop\n",
    "# X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "\n",
    "size_train_data = len(train_data) - block_size\n",
    "\n",
    "while True:\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['train'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['train']\n",
    "            if iter_num >= 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "                \n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "    \n",
    "    X, Y = get_batch('train', iter_num % size_train_data)\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, epoch {int(iter_num/size_train_data)}, index {iter_num%size_train_data}\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
