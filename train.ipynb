{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out-stock'\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 20\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'stock'\n",
    "wandb_run_name = 'mini-gpt' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'stock'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 16\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 1024\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "meta_vocab_size = 512\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 30000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3095, 1024])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "datadir = os.path.join('data', dataset)\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(datadir, 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join(datadir, 'val.csv'))\n",
    "\n",
    "# 将NumPy数组转换为pandas DataFrame\n",
    "train_data = torch.tensor(train_data.values, dtype=torch.float16)[:, 1:n_embd+1]\n",
    "val_data = torch.tensor(val_data.values, dtype=torch.float16)[:, 1:n_embd+1]\n",
    "print(train_data.shape)\n",
    "torch.manual_seed(33)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    indices = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    y_indices = [torch.arange(0, len(x[i][0]) // 2) for i, j in enumerate(indices)]\n",
    "    \n",
    "    y_list = []\n",
    "    for i, j in enumerate(indices):\n",
    "        next_row = data[j + 1:j + 1 + block_size]\n",
    "        price_chg_cols = [2 * col + 1 for col in y_indices[i]]\n",
    "        \n",
    "        topk = 10\n",
    "        topk_values, topk_indices = next_row[:, price_chg_cols].topk(topk, dim=1)\n",
    "\n",
    "        random_indices = torch.randint(0, topk, (topk_indices.size(0),))\n",
    "        selected_indices = topk_indices[torch.arange(topk_indices.size(0)), random_indices]\n",
    "\n",
    "        y_list.append(selected_indices.clone().detach())\n",
    "        \n",
    "\n",
    "    y = torch.stack(y_list)\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "number of parameters: 50.87M\n",
      "num decayed parameter tensors: 18, with 50,872,320 parameters\n",
      "num non-decayed parameter tensors: 9, with 9,216 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ren/miniconda3/envs/nanogpt/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/home/ren/miniconda3/envs/nanogpt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n",
      "logits is tensor([[[ 1.0088, -0.1734,  0.5224,  ...,  0.1623, -0.5702,  0.0028],\n",
      "         [ 0.2886,  1.1159, -0.2300,  ..., -0.2401,  0.0700, -1.2361],\n",
      "         [-1.8174,  0.8377, -0.9774,  ..., -0.1676, -0.1522, -0.1861],\n",
      "         ...,\n",
      "         [-0.2395,  0.1944, -1.3538,  ...,  1.2134, -0.0032, -0.6998],\n",
      "         [ 1.0197,  0.3690, -1.3425,  ...,  0.2541,  0.7447, -0.5157],\n",
      "         [-0.0682, -0.0148, -0.5980,  ..., -0.7264,  0.0043, -0.2871]],\n",
      "\n",
      "        [[ 0.3299,  0.9735,  0.8958,  ..., -0.1359,  0.3777, -0.3676],\n",
      "         [-1.2233,  0.0434,  0.6422,  ..., -0.4488,  0.3291, -0.0527],\n",
      "         [ 0.6413, -0.1127,  1.0667,  ..., -0.0047,  0.4245, -0.3197],\n",
      "         ...,\n",
      "         [-0.4302,  0.2473, -0.0332,  ..., -1.3657,  0.2179,  1.0003],\n",
      "         [ 0.1828, -1.0715, -0.8466,  ...,  1.0277, -0.0102,  0.2144],\n",
      "         [ 0.7611, -0.3018,  0.1260,  ..., -0.2015,  0.7305, -1.4763]],\n",
      "\n",
      "        [[-1.3103, -0.9692,  0.3495,  ...,  0.1735, -0.1471, -0.3254],\n",
      "         [ 0.6212,  0.1213,  0.2089,  ..., -0.7513, -0.2585, -0.3246],\n",
      "         [ 0.6339, -0.0482, -0.7296,  ...,  0.0174, -0.3168, -0.2230],\n",
      "         ...,\n",
      "         [-0.5611, -0.2970, -0.2104,  ..., -0.8078, -1.0726, -0.3385],\n",
      "         [ 1.5206, -0.7735,  0.8391,  ...,  1.7948,  0.6108, -1.3125],\n",
      "         [ 1.0452, -0.4970,  0.9764,  ...,  0.9986, -0.3193, -0.9463]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2459,  0.4809, -0.6347,  ...,  0.7821, -0.4134, -0.0653],\n",
      "         [ 1.6016,  1.2655, -0.5036,  ..., -1.2461,  0.0264, -0.8456],\n",
      "         [ 0.2297,  0.1648, -0.1594,  ...,  0.3698, -0.2279, -0.1376],\n",
      "         ...,\n",
      "         [ 0.2496,  0.2391, -0.9513,  ...,  0.7296, -0.5406, -0.4457],\n",
      "         [-0.1829,  0.3076, -0.4440,  ...,  0.8996,  0.7348,  0.1917],\n",
      "         [-0.2874,  0.1034, -0.3831,  ..., -0.1337, -0.7065, -0.7102]],\n",
      "\n",
      "        [[-0.4569, -0.9140, -0.1668,  ...,  0.2202,  0.7372,  0.2305],\n",
      "         [-0.0721, -1.3161, -0.1907,  ...,  0.5454, -0.7199,  0.2565],\n",
      "         [ 0.2002, -0.4681,  0.7996,  ..., -0.0619, -0.1347, -0.5175],\n",
      "         ...,\n",
      "         [-0.7665, -0.8739, -0.8764,  ..., -0.4421, -0.5984, -0.2890],\n",
      "         [ 0.6342,  0.3177, -0.2828,  ..., -0.6117,  0.2147, -0.3250],\n",
      "         [-0.4929,  0.0680, -0.6637,  ..., -0.1883,  0.4911, -0.4944]],\n",
      "\n",
      "        [[ 1.1638, -0.4539, -0.3796,  ..., -0.3886,  1.1403, -0.6228],\n",
      "         [-0.5518,  0.3157,  0.2699,  ..., -0.3907,  0.1508, -0.5031],\n",
      "         [ 0.2218,  0.7466, -0.1041,  ...,  0.7206,  0.0990,  0.5351],\n",
      "         ...,\n",
      "         [ 1.3367, -0.8565,  0.5072,  ...,  1.4330,  0.4566, -0.2561],\n",
      "         [-0.1366, -2.0528,  0.7034,  ..., -0.4611, -0.2810, -0.2541],\n",
      "         [-0.2049, -0.1434, -0.8249,  ..., -0.3208,  0.7754,  0.3505]]],\n",
      "       grad_fn=<UnsafeViewBackward0>), shape is torch.Size([12, 16, 512])\n",
      "loss is 6.442887783050537, loss.shape is torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=meta_vocab_size, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "\n",
    "\n",
    "\n",
    "logits, loss = model(X, Y)\n",
    "print(f'logits is {logits}, shape is {logits.shape}')\n",
    "print(f'loss is {loss}, loss.shape is {loss.shape}')\n",
    "# print(f'logits={logits}')\n",
    "\n",
    "# Y.shape\n",
    "# logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.4272, val loss 6.4567\n",
      "iter 0: loss 6.3955, time 5360.83ms, mfu -100.00%\n",
      "iter 1: loss 6.4582, time 538.95ms, mfu -100.00%\n",
      "iter 2: loss 6.3974, time 535.46ms, mfu -100.00%\n",
      "iter 3: loss 6.4416, time 523.71ms, mfu -100.00%\n",
      "iter 4: loss 6.3736, time 544.76ms, mfu -100.00%\n",
      "iter 5: loss 6.4316, time 519.61ms, mfu 0.04%\n",
      "iter 6: loss 6.3863, time 446.85ms, mfu 0.04%\n",
      "iter 7: loss 6.3482, time 505.20ms, mfu 0.04%\n",
      "iter 8: loss 6.4016, time 603.47ms, mfu 0.04%\n",
      "iter 9: loss 6.2818, time 572.73ms, mfu 0.04%\n",
      "iter 10: loss 6.3115, time 582.90ms, mfu 0.04%\n",
      "iter 11: loss 6.3974, time 806.18ms, mfu 0.03%\n",
      "iter 12: loss 6.4362, time 788.29ms, mfu 0.03%\n",
      "iter 13: loss 6.3132, time 755.73ms, mfu 0.03%\n",
      "iter 14: loss 6.3049, time 739.13ms, mfu 0.03%\n",
      "iter 15: loss 6.3593, time 569.49ms, mfu 0.03%\n",
      "iter 16: loss 6.2651, time 604.75ms, mfu 0.03%\n",
      "iter 17: loss 6.2322, time 616.05ms, mfu 0.03%\n",
      "iter 18: loss 6.3474, time 727.66ms, mfu 0.03%\n",
      "iter 19: loss 6.3349, time 579.43ms, mfu 0.03%\n",
      "iter 20: loss 6.1784, time 590.87ms, mfu 0.03%\n",
      "iter 21: loss 6.1758, time 580.56ms, mfu 0.03%\n",
      "iter 22: loss 6.1420, time 652.66ms, mfu 0.03%\n",
      "iter 23: loss 6.3460, time 584.43ms, mfu 0.03%\n",
      "iter 24: loss 6.2596, time 578.75ms, mfu 0.03%\n",
      "iter 25: loss 6.2759, time 592.26ms, mfu 0.03%\n",
      "iter 26: loss 6.5080, time 826.07ms, mfu 0.03%\n",
      "iter 27: loss 6.3226, time 816.57ms, mfu 0.03%\n",
      "iter 28: loss 6.4076, time 751.60ms, mfu 0.03%\n",
      "iter 29: loss 6.0608, time 566.52ms, mfu 0.03%\n",
      "iter 30: loss 6.1215, time 551.93ms, mfu 0.03%\n",
      "iter 31: loss 6.2534, time 585.26ms, mfu 0.03%\n",
      "iter 32: loss 6.3615, time 554.92ms, mfu 0.03%\n",
      "iter 33: loss 6.3189, time 578.62ms, mfu 0.03%\n",
      "iter 34: loss 6.1248, time 785.59ms, mfu 0.03%\n",
      "iter 35: loss 6.1934, time 759.79ms, mfu 0.03%\n",
      "iter 36: loss 6.2798, time 755.16ms, mfu 0.03%\n",
      "iter 37: loss 6.1246, time 683.62ms, mfu 0.03%\n",
      "iter 38: loss 6.1865, time 568.79ms, mfu 0.03%\n",
      "iter 39: loss 6.3476, time 593.37ms, mfu 0.03%\n",
      "iter 40: loss 6.0856, time 761.86ms, mfu 0.03%\n",
      "iter 41: loss 6.1253, time 799.45ms, mfu 0.03%\n",
      "iter 42: loss 6.1397, time 743.71ms, mfu 0.03%\n",
      "iter 43: loss 6.0621, time 658.54ms, mfu 0.03%\n",
      "iter 44: loss 6.1523, time 588.13ms, mfu 0.03%\n",
      "iter 45: loss 6.2667, time 709.62ms, mfu 0.03%\n",
      "iter 46: loss 6.0521, time 760.87ms, mfu 0.03%\n",
      "iter 47: loss 6.1924, time 627.41ms, mfu 0.03%\n",
      "iter 48: loss 5.9656, time 731.74ms, mfu 0.03%\n",
      "iter 49: loss 6.1655, time 762.88ms, mfu 0.03%\n",
      "iter 50: loss 5.9516, time 746.50ms, mfu 0.03%\n",
      "iter 51: loss 6.1011, time 550.90ms, mfu 0.03%\n",
      "iter 52: loss 6.1398, time 556.11ms, mfu 0.03%\n",
      "iter 53: loss 6.1652, time 543.53ms, mfu 0.03%\n",
      "iter 54: loss 6.1500, time 541.91ms, mfu 0.03%\n",
      "iter 55: loss 6.0455, time 586.82ms, mfu 0.03%\n",
      "iter 56: loss 5.8579, time 580.64ms, mfu 0.03%\n",
      "iter 57: loss 6.1593, time 562.28ms, mfu 0.03%\n",
      "iter 58: loss 6.2062, time 543.39ms, mfu 0.03%\n",
      "iter 59: loss 6.1087, time 709.39ms, mfu 0.03%\n",
      "iter 60: loss 6.1579, time 731.12ms, mfu 0.03%\n",
      "iter 61: loss 5.9269, time 609.22ms, mfu 0.03%\n",
      "iter 62: loss 6.0794, time 749.91ms, mfu 0.03%\n",
      "iter 63: loss 5.8210, time 697.19ms, mfu 0.03%\n",
      "iter 64: loss 6.1014, time 745.69ms, mfu 0.03%\n",
      "iter 65: loss 6.0442, time 714.35ms, mfu 0.03%\n",
      "iter 66: loss 6.1493, time 713.44ms, mfu 0.03%\n",
      "iter 67: loss 5.9650, time 710.87ms, mfu 0.03%\n",
      "iter 68: loss 6.1103, time 721.15ms, mfu 0.03%\n",
      "iter 69: loss 6.0172, time 735.91ms, mfu 0.03%\n",
      "iter 70: loss 5.9400, time 766.44ms, mfu 0.03%\n",
      "iter 71: loss 6.1200, time 655.94ms, mfu 0.03%\n",
      "iter 72: loss 6.1337, time 549.50ms, mfu 0.03%\n",
      "iter 73: loss 6.0563, time 555.43ms, mfu 0.03%\n",
      "iter 74: loss 5.9744, time 561.15ms, mfu 0.03%\n",
      "iter 75: loss 6.1796, time 572.29ms, mfu 0.03%\n",
      "iter 76: loss 6.2235, time 626.03ms, mfu 0.03%\n",
      "iter 77: loss 5.9762, time 553.24ms, mfu 0.03%\n",
      "iter 78: loss 6.3160, time 571.63ms, mfu 0.03%\n",
      "iter 79: loss 6.0100, time 586.56ms, mfu 0.03%\n",
      "iter 80: loss 6.0068, time 581.52ms, mfu 0.03%\n",
      "iter 81: loss 6.0222, time 554.96ms, mfu 0.03%\n",
      "iter 82: loss 6.2858, time 589.14ms, mfu 0.03%\n",
      "iter 83: loss 6.1943, time 751.10ms, mfu 0.03%\n",
      "iter 84: loss 5.9810, time 720.97ms, mfu 0.03%\n",
      "iter 85: loss 5.9862, time 622.72ms, mfu 0.03%\n",
      "iter 86: loss 5.8969, time 542.52ms, mfu 0.03%\n",
      "iter 87: loss 6.1361, time 660.82ms, mfu 0.03%\n",
      "iter 88: loss 5.7946, time 561.04ms, mfu 0.03%\n",
      "iter 89: loss 5.9502, time 584.93ms, mfu 0.03%\n",
      "iter 90: loss 6.2889, time 558.97ms, mfu 0.03%\n",
      "iter 91: loss 6.1121, time 525.58ms, mfu 0.03%\n",
      "iter 92: loss 5.9594, time 530.99ms, mfu 0.03%\n",
      "iter 93: loss 6.2234, time 570.39ms, mfu 0.03%\n",
      "iter 94: loss 6.2225, time 547.03ms, mfu 0.03%\n",
      "iter 95: loss 6.2386, time 571.54ms, mfu 0.03%\n",
      "iter 96: loss 6.3458, time 619.77ms, mfu 0.03%\n",
      "iter 97: loss 6.1611, time 779.25ms, mfu 0.03%\n",
      "iter 98: loss 6.0479, time 718.73ms, mfu 0.03%\n",
      "iter 99: loss 6.2507, time 714.10ms, mfu 0.03%\n",
      "step 100: train loss 6.1372, val loss 6.7160\n",
      "iter 100: loss 6.2958, time 7141.31ms, mfu 0.03%\n",
      "iter 101: loss 6.4270, time 609.68ms, mfu 0.03%\n",
      "iter 102: loss 6.0932, time 667.42ms, mfu 0.03%\n",
      "iter 103: loss 6.1869, time 680.11ms, mfu 0.03%\n",
      "iter 104: loss 6.0912, time 560.69ms, mfu 0.03%\n",
      "iter 105: loss 6.0994, time 577.24ms, mfu 0.03%\n",
      "iter 106: loss 6.2467, time 554.45ms, mfu 0.03%\n",
      "iter 107: loss 6.1826, time 688.57ms, mfu 0.03%\n",
      "iter 108: loss 6.2237, time 733.75ms, mfu 0.03%\n",
      "iter 109: loss 6.2546, time 773.13ms, mfu 0.03%\n",
      "iter 110: loss 6.2044, time 573.13ms, mfu 0.03%\n",
      "iter 111: loss 6.1203, time 663.56ms, mfu 0.03%\n",
      "iter 112: loss 6.0040, time 662.52ms, mfu 0.03%\n",
      "iter 113: loss 6.1579, time 709.27ms, mfu 0.03%\n",
      "iter 114: loss 6.2288, time 520.35ms, mfu 0.03%\n",
      "iter 115: loss 5.9773, time 573.15ms, mfu 0.03%\n",
      "iter 116: loss 6.0195, time 606.16ms, mfu 0.03%\n",
      "iter 117: loss 6.1817, time 650.45ms, mfu 0.03%\n",
      "iter 118: loss 6.2016, time 564.75ms, mfu 0.03%\n",
      "iter 119: loss 6.3603, time 768.24ms, mfu 0.03%\n",
      "iter 120: loss 6.3430, time 669.12ms, mfu 0.03%\n",
      "iter 121: loss 6.1108, time 568.07ms, mfu 0.03%\n",
      "iter 122: loss 6.1275, time 781.65ms, mfu 0.03%\n",
      "iter 123: loss 6.1926, time 767.47ms, mfu 0.03%\n",
      "iter 124: loss 6.1068, time 630.93ms, mfu 0.03%\n",
      "iter 125: loss 6.0250, time 606.47ms, mfu 0.03%\n",
      "iter 126: loss 6.2031, time 603.31ms, mfu 0.03%\n",
      "iter 127: loss 6.2610, time 603.60ms, mfu 0.03%\n",
      "iter 128: loss 6.0089, time 597.35ms, mfu 0.03%\n",
      "iter 129: loss 6.3954, time 577.92ms, mfu 0.03%\n",
      "iter 130: loss 6.0324, time 553.92ms, mfu 0.03%\n",
      "iter 131: loss 6.1786, time 557.10ms, mfu 0.03%\n",
      "iter 132: loss 6.3209, time 560.76ms, mfu 0.03%\n",
      "iter 133: loss 6.0838, time 589.13ms, mfu 0.03%\n",
      "iter 134: loss 6.2222, time 695.50ms, mfu 0.03%\n",
      "iter 135: loss 6.2213, time 569.36ms, mfu 0.03%\n",
      "iter 136: loss 6.1994, time 558.88ms, mfu 0.03%\n",
      "iter 137: loss 6.2623, time 598.71ms, mfu 0.03%\n",
      "iter 138: loss 6.2112, time 799.74ms, mfu 0.03%\n",
      "iter 139: loss 6.0960, time 774.00ms, mfu 0.03%\n",
      "iter 140: loss 6.2560, time 869.36ms, mfu 0.03%\n",
      "iter 141: loss 6.0300, time 812.17ms, mfu 0.03%\n",
      "iter 142: loss 6.1134, time 764.70ms, mfu 0.03%\n",
      "iter 143: loss 6.2980, time 766.74ms, mfu 0.03%\n",
      "iter 144: loss 6.0468, time 744.88ms, mfu 0.03%\n",
      "iter 145: loss 6.3233, time 774.34ms, mfu 0.03%\n",
      "iter 146: loss 6.1038, time 556.29ms, mfu 0.03%\n",
      "iter 147: loss 6.1677, time 602.03ms, mfu 0.03%\n",
      "iter 148: loss 6.1254, time 594.92ms, mfu 0.03%\n",
      "iter 149: loss 6.2376, time 551.56ms, mfu 0.03%\n",
      "iter 150: loss 6.1610, time 557.29ms, mfu 0.03%\n",
      "iter 151: loss 6.1501, time 582.31ms, mfu 0.03%\n",
      "iter 152: loss 6.1752, time 624.40ms, mfu 0.03%\n",
      "iter 153: loss 6.0399, time 650.25ms, mfu 0.03%\n",
      "iter 154: loss 6.1246, time 796.05ms, mfu 0.03%\n",
      "iter 155: loss 6.1071, time 816.79ms, mfu 0.03%\n",
      "iter 156: loss 6.1183, time 734.14ms, mfu 0.03%\n",
      "iter 157: loss 6.1033, time 762.13ms, mfu 0.03%\n",
      "iter 158: loss 6.0734, time 733.79ms, mfu 0.03%\n",
      "iter 159: loss 6.2716, time 692.83ms, mfu 0.03%\n",
      "iter 160: loss 6.1067, time 547.09ms, mfu 0.03%\n",
      "iter 161: loss 6.3239, time 626.80ms, mfu 0.03%\n",
      "iter 162: loss 6.0409, time 580.00ms, mfu 0.03%\n",
      "iter 163: loss 6.1151, time 593.73ms, mfu 0.03%\n",
      "iter 164: loss 6.4070, time 633.22ms, mfu 0.03%\n",
      "iter 165: loss 6.1780, time 770.26ms, mfu 0.03%\n",
      "iter 166: loss 5.9920, time 736.20ms, mfu 0.03%\n",
      "iter 167: loss 6.1329, time 720.32ms, mfu 0.03%\n",
      "iter 168: loss 6.0359, time 752.34ms, mfu 0.03%\n",
      "iter 169: loss 6.0266, time 800.28ms, mfu 0.03%\n",
      "iter 170: loss 5.9564, time 774.00ms, mfu 0.03%\n",
      "iter 171: loss 6.2006, time 740.53ms, mfu 0.03%\n",
      "iter 172: loss 6.2022, time 725.27ms, mfu 0.03%\n",
      "iter 173: loss 6.1657, time 724.48ms, mfu 0.03%\n",
      "iter 174: loss 6.0953, time 741.51ms, mfu 0.03%\n",
      "iter 175: loss 6.1402, time 650.45ms, mfu 0.03%\n",
      "iter 176: loss 6.2107, time 662.06ms, mfu 0.03%\n",
      "iter 177: loss 5.9794, time 609.11ms, mfu 0.03%\n",
      "iter 178: loss 6.0542, time 589.58ms, mfu 0.03%\n",
      "iter 179: loss 6.0744, time 606.83ms, mfu 0.03%\n",
      "iter 180: loss 6.2184, time 593.11ms, mfu 0.03%\n",
      "iter 181: loss 6.0907, time 601.87ms, mfu 0.03%\n",
      "iter 182: loss 6.2071, time 779.27ms, mfu 0.03%\n",
      "iter 183: loss 6.0354, time 766.20ms, mfu 0.03%\n",
      "iter 184: loss 6.0793, time 774.31ms, mfu 0.03%\n",
      "iter 185: loss 6.0124, time 789.24ms, mfu 0.03%\n",
      "iter 186: loss 6.0406, time 749.66ms, mfu 0.03%\n",
      "iter 187: loss 5.9879, time 609.84ms, mfu 0.03%\n",
      "iter 188: loss 5.9217, time 548.17ms, mfu 0.03%\n",
      "iter 189: loss 6.0502, time 633.93ms, mfu 0.03%\n",
      "iter 190: loss 5.9734, time 805.04ms, mfu 0.03%\n",
      "iter 191: loss 5.6613, time 812.42ms, mfu 0.03%\n",
      "iter 192: loss 6.0269, time 763.48ms, mfu 0.03%\n",
      "iter 193: loss 5.9344, time 759.10ms, mfu 0.03%\n",
      "iter 194: loss 5.8817, time 658.59ms, mfu 0.03%\n",
      "iter 195: loss 6.0622, time 588.93ms, mfu 0.03%\n",
      "iter 196: loss 6.0212, time 601.94ms, mfu 0.03%\n",
      "iter 197: loss 6.0549, time 795.96ms, mfu 0.03%\n",
      "iter 198: loss 6.1302, time 750.24ms, mfu 0.03%\n",
      "iter 199: loss 6.0110, time 778.03ms, mfu 0.03%\n",
      "step 200: train loss 5.9943, val loss 6.5243\n",
      "iter 200: loss 6.1754, time 7889.07ms, mfu 0.02%\n",
      "iter 201: loss 5.8659, time 581.37ms, mfu 0.03%\n",
      "iter 202: loss 6.0495, time 831.23ms, mfu 0.02%\n",
      "iter 203: loss 5.9252, time 759.43ms, mfu 0.02%\n",
      "iter 204: loss 6.0816, time 773.61ms, mfu 0.02%\n",
      "iter 205: loss 5.9773, time 576.59ms, mfu 0.03%\n",
      "iter 206: loss 5.9958, time 574.25ms, mfu 0.03%\n",
      "iter 207: loss 5.9669, time 601.68ms, mfu 0.03%\n",
      "iter 208: loss 5.9677, time 588.41ms, mfu 0.03%\n",
      "iter 209: loss 5.6077, time 651.17ms, mfu 0.03%\n",
      "iter 210: loss 5.8866, time 622.49ms, mfu 0.03%\n",
      "iter 211: loss 5.7147, time 576.40ms, mfu 0.03%\n",
      "iter 212: loss 6.0261, time 576.48ms, mfu 0.03%\n",
      "iter 213: loss 5.7688, time 598.30ms, mfu 0.03%\n",
      "iter 214: loss 6.0166, time 570.78ms, mfu 0.03%\n",
      "iter 215: loss 6.0457, time 597.25ms, mfu 0.03%\n",
      "iter 216: loss 5.9276, time 783.36ms, mfu 0.03%\n",
      "iter 217: loss 6.0826, time 805.43ms, mfu 0.03%\n",
      "iter 218: loss 6.0938, time 750.01ms, mfu 0.03%\n",
      "iter 219: loss 5.8777, time 776.64ms, mfu 0.03%\n",
      "iter 220: loss 5.9591, time 759.57ms, mfu 0.03%\n",
      "iter 221: loss 5.8475, time 750.09ms, mfu 0.03%\n",
      "iter 222: loss 5.8484, time 846.99ms, mfu 0.03%\n",
      "iter 223: loss 5.8126, time 1166.82ms, mfu 0.03%\n",
      "iter 224: loss 5.6823, time 771.61ms, mfu 0.03%\n",
      "iter 225: loss 5.9351, time 638.04ms, mfu 0.03%\n",
      "iter 226: loss 5.9226, time 608.98ms, mfu 0.03%\n",
      "iter 227: loss 5.9394, time 586.81ms, mfu 0.03%\n",
      "iter 228: loss 5.8198, time 652.01ms, mfu 0.03%\n",
      "iter 229: loss 5.7858, time 585.06ms, mfu 0.03%\n",
      "iter 230: loss 5.9626, time 629.68ms, mfu 0.03%\n",
      "iter 231: loss 5.8091, time 706.70ms, mfu 0.03%\n",
      "iter 232: loss 5.7619, time 617.68ms, mfu 0.03%\n",
      "iter 233: loss 5.7965, time 619.28ms, mfu 0.03%\n",
      "iter 234: loss 5.8979, time 599.81ms, mfu 0.03%\n",
      "iter 235: loss 5.8838, time 803.92ms, mfu 0.03%\n",
      "iter 236: loss 5.8162, time 824.10ms, mfu 0.03%\n",
      "iter 237: loss 5.9105, time 995.85ms, mfu 0.03%\n",
      "iter 238: loss 5.6838, time 876.05ms, mfu 0.03%\n",
      "iter 239: loss 5.7736, time 821.04ms, mfu 0.03%\n",
      "iter 240: loss 5.8678, time 815.11ms, mfu 0.03%\n",
      "iter 241: loss 6.0725, time 806.92ms, mfu 0.03%\n",
      "iter 242: loss 5.9614, time 759.15ms, mfu 0.03%\n",
      "iter 243: loss 6.0586, time 784.37ms, mfu 0.03%\n",
      "iter 244: loss 5.8445, time 925.00ms, mfu 0.02%\n",
      "iter 245: loss 5.8131, time 854.57ms, mfu 0.02%\n",
      "iter 246: loss 5.7386, time 816.96ms, mfu 0.02%\n",
      "iter 247: loss 5.8384, time 551.80ms, mfu 0.03%\n",
      "iter 248: loss 5.7977, time 592.20ms, mfu 0.03%\n",
      "iter 249: loss 5.8049, time 582.60ms, mfu 0.03%\n",
      "iter 250: loss 5.9650, time 542.21ms, mfu 0.03%\n",
      "iter 251: loss 5.8309, time 587.53ms, mfu 0.03%\n",
      "iter 252: loss 5.9615, time 603.77ms, mfu 0.03%\n",
      "iter 253: loss 5.9291, time 614.42ms, mfu 0.03%\n",
      "iter 254: loss 5.8388, time 584.84ms, mfu 0.03%\n",
      "iter 255: loss 5.6762, time 757.77ms, mfu 0.03%\n",
      "iter 256: loss 5.8426, time 753.98ms, mfu 0.03%\n",
      "iter 257: loss 5.9506, time 738.60ms, mfu 0.03%\n",
      "iter 258: loss 6.3026, time 532.26ms, mfu 0.03%\n",
      "iter 259: loss 5.8673, time 651.26ms, mfu 0.03%\n",
      "iter 260: loss 5.8336, time 602.78ms, mfu 0.03%\n",
      "iter 261: loss 5.7941, time 597.17ms, mfu 0.03%\n",
      "iter 262: loss 6.0336, time 587.11ms, mfu 0.03%\n",
      "iter 263: loss 5.8830, time 583.94ms, mfu 0.03%\n",
      "iter 264: loss 5.7864, time 612.00ms, mfu 0.03%\n",
      "iter 265: loss 5.9083, time 588.33ms, mfu 0.03%\n",
      "iter 266: loss 5.9624, time 586.74ms, mfu 0.03%\n",
      "iter 267: loss 5.8162, time 628.30ms, mfu 0.03%\n",
      "iter 268: loss 5.7890, time 572.56ms, mfu 0.03%\n",
      "iter 269: loss 5.8879, time 591.58ms, mfu 0.03%\n",
      "iter 270: loss 5.7473, time 579.48ms, mfu 0.03%\n",
      "iter 271: loss 5.8671, time 566.88ms, mfu 0.03%\n",
      "iter 272: loss 5.8479, time 579.98ms, mfu 0.03%\n",
      "iter 273: loss 5.7279, time 594.90ms, mfu 0.03%\n",
      "iter 274: loss 5.9256, time 589.13ms, mfu 0.03%\n",
      "iter 275: loss 5.9002, time 591.13ms, mfu 0.03%\n",
      "iter 276: loss 5.8055, time 562.12ms, mfu 0.03%\n",
      "iter 277: loss 5.8502, time 767.91ms, mfu 0.03%\n",
      "iter 278: loss 5.9011, time 715.07ms, mfu 0.03%\n",
      "iter 279: loss 5.7755, time 541.55ms, mfu 0.03%\n",
      "iter 280: loss 5.9327, time 561.83ms, mfu 0.03%\n",
      "iter 281: loss 5.8664, time 572.53ms, mfu 0.03%\n",
      "iter 282: loss 5.7107, time 565.46ms, mfu 0.03%\n",
      "iter 283: loss 5.9449, time 559.04ms, mfu 0.03%\n",
      "iter 284: loss 5.7747, time 595.27ms, mfu 0.03%\n",
      "iter 285: loss 5.7106, time 697.54ms, mfu 0.03%\n",
      "iter 286: loss 5.8620, time 688.93ms, mfu 0.03%\n",
      "iter 287: loss 5.7353, time 566.27ms, mfu 0.03%\n",
      "iter 288: loss 5.8175, time 589.83ms, mfu 0.03%\n",
      "iter 289: loss 5.8556, time 581.69ms, mfu 0.03%\n",
      "iter 290: loss 5.8907, time 576.82ms, mfu 0.03%\n",
      "iter 291: loss 5.7859, time 571.10ms, mfu 0.03%\n",
      "iter 292: loss 5.8029, time 586.67ms, mfu 0.03%\n",
      "iter 293: loss 5.8042, time 582.64ms, mfu 0.03%\n",
      "iter 294: loss 5.8863, time 577.98ms, mfu 0.03%\n",
      "iter 295: loss 5.5352, time 576.42ms, mfu 0.03%\n",
      "iter 296: loss 5.8672, time 554.99ms, mfu 0.03%\n",
      "iter 297: loss 5.8254, time 584.03ms, mfu 0.03%\n",
      "iter 298: loss 5.7313, time 576.44ms, mfu 0.03%\n",
      "iter 299: loss 5.8288, time 630.24ms, mfu 0.03%\n",
      "step 300: train loss 5.7173, val loss 6.5310\n",
      "iter 300: loss 5.6961, time 8099.55ms, mfu 0.03%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
