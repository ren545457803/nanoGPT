{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out-stock'\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_iters = 2\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'stock'\n",
    "wandb_run_name = 'mini-gpt' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'stock'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 5\n",
    "# model\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 512 # 参数估计：params= layer*(12*emb**2), 数据量估计dataSize= 10 *params\n",
    "# dropout = 0.998728434 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "dropout = 0\n",
    "# dropout = 1-dataSize/10/params\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "meta_vocab_size = n_embd // 2\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 1000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_date</th>\n",
       "      <th>000001.SZ</th>\n",
       "      <th>000002.SZ</th>\n",
       "      <th>000004.SZ</th>\n",
       "      <th>000005.SZ</th>\n",
       "      <th>000006.SZ</th>\n",
       "      <th>000007.SZ</th>\n",
       "      <th>000008.SZ</th>\n",
       "      <th>000009.SZ</th>\n",
       "      <th>000010.SZ</th>\n",
       "      <th>...</th>\n",
       "      <th>603320.SH</th>\n",
       "      <th>603321.SH</th>\n",
       "      <th>603322.SH</th>\n",
       "      <th>603323.SH</th>\n",
       "      <th>603324.SH</th>\n",
       "      <th>603325.SH</th>\n",
       "      <th>603326.SH</th>\n",
       "      <th>603327.SH</th>\n",
       "      <th>603328.SH</th>\n",
       "      <th>603329.SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20240206</td>\n",
       "      <td>1.0320</td>\n",
       "      <td>1.0359</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>0.9506</td>\n",
       "      <td>1.0420</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>1.0412</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>1.0424</td>\n",
       "      <td>1.0339</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>1.0645</td>\n",
       "      <td>0.9214</td>\n",
       "      <td>1.0366</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20240207</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>1.0011</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9481</td>\n",
       "      <td>1.0161</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>1.0991</td>\n",
       "      <td>0.9036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.0211</td>\n",
       "      <td>0.9627</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>0.9729</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.0999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240208</td>\n",
       "      <td>1.0062</td>\n",
       "      <td>1.0567</td>\n",
       "      <td>1.0559</td>\n",
       "      <td>0.9452</td>\n",
       "      <td>1.0714</td>\n",
       "      <td>1.0308</td>\n",
       "      <td>1.0350</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>1.0955</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.1008</td>\n",
       "      <td>1.0954</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>1.0887</td>\n",
       "      <td>1.0727</td>\n",
       "      <td>1.0967</td>\n",
       "      <td>1.0923</td>\n",
       "      <td>1.0387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20240219</td>\n",
       "      <td>1.0145</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>1.0800</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9852</td>\n",
       "      <td>1.0149</td>\n",
       "      <td>1.0145</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>1.0359</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>1.0534</td>\n",
       "      <td>1.1001</td>\n",
       "      <td>1.0138</td>\n",
       "      <td>1.0440</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0610</td>\n",
       "      <td>1.0639</td>\n",
       "      <td>1.0096</td>\n",
       "      <td>0.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20240220</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>1.0061</td>\n",
       "      <td>1.0144</td>\n",
       "      <td>1.0455</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>1.0490</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0396</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0306</td>\n",
       "      <td>1.0127</td>\n",
       "      <td>1.0618</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>1.0289</td>\n",
       "      <td>1.0154</td>\n",
       "      <td>1.0176</td>\n",
       "      <td>1.0062</td>\n",
       "      <td>1.0047</td>\n",
       "      <td>1.0130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20240221</td>\n",
       "      <td>1.0998</td>\n",
       "      <td>1.0313</td>\n",
       "      <td>1.0218</td>\n",
       "      <td>1.0435</td>\n",
       "      <td>1.0100</td>\n",
       "      <td>1.0023</td>\n",
       "      <td>1.0096</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>1.0429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0248</td>\n",
       "      <td>1.0233</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.0226</td>\n",
       "      <td>1.0052</td>\n",
       "      <td>1.0249</td>\n",
       "      <td>1.0565</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20240222</td>\n",
       "      <td>1.0093</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>1.0455</td>\n",
       "      <td>1.0556</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>1.0047</td>\n",
       "      <td>1.0102</td>\n",
       "      <td>1.0137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0502</td>\n",
       "      <td>1.0385</td>\n",
       "      <td>1.0865</td>\n",
       "      <td>1.0133</td>\n",
       "      <td>1.0189</td>\n",
       "      <td>1.0016</td>\n",
       "      <td>1.0193</td>\n",
       "      <td>1.0237</td>\n",
       "      <td>1.0220</td>\n",
       "      <td>1.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20240223</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>1.0049</td>\n",
       "      <td>1.0586</td>\n",
       "      <td>1.0526</td>\n",
       "      <td>1.0099</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>1.0142</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0524</td>\n",
       "      <td>1.0589</td>\n",
       "      <td>1.0221</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>1.0253</td>\n",
       "      <td>1.0271</td>\n",
       "      <td>1.0641</td>\n",
       "      <td>1.0241</td>\n",
       "      <td>1.0231</td>\n",
       "      <td>1.0149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20240226</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>1.0335</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>1.0267</td>\n",
       "      <td>1.0047</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>1.0216</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0315</td>\n",
       "      <td>1.0795</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>1.0107</td>\n",
       "      <td>1.0133</td>\n",
       "      <td>1.0014</td>\n",
       "      <td>1.0186</td>\n",
       "      <td>1.0120</td>\n",
       "      <td>1.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20240227</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>1.0080</td>\n",
       "      <td>1.0998</td>\n",
       "      <td>1.0476</td>\n",
       "      <td>1.0248</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>1.0093</td>\n",
       "      <td>1.0101</td>\n",
       "      <td>1.0127</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0347</td>\n",
       "      <td>1.0103</td>\n",
       "      <td>1.0341</td>\n",
       "      <td>1.0022</td>\n",
       "      <td>1.0460</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>1.0274</td>\n",
       "      <td>1.0308</td>\n",
       "      <td>1.0461</td>\n",
       "      <td>1.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20240228</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.8997</td>\n",
       "      <td>1.0455</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9001</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>1.0039</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>0.9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20240229</td>\n",
       "      <td>1.0095</td>\n",
       "      <td>1.0111</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>1.0543</td>\n",
       "      <td>1.0175</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>1.0206</td>\n",
       "      <td>1.0586</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0155</td>\n",
       "      <td>1.0268</td>\n",
       "      <td>1.0589</td>\n",
       "      <td>1.0136</td>\n",
       "      <td>1.0561</td>\n",
       "      <td>1.0302</td>\n",
       "      <td>1.0355</td>\n",
       "      <td>1.0998</td>\n",
       "      <td>1.0472</td>\n",
       "      <td>1.0308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20240301</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>1.0222</td>\n",
       "      <td>0.9874</td>\n",
       "      <td>1.0047</td>\n",
       "      <td>1.0101</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0036</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>1.0075</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0191</td>\n",
       "      <td>1.0400</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.1003</td>\n",
       "      <td>1.0131</td>\n",
       "      <td>0.9868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20240304</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>1.0242</td>\n",
       "      <td>0.9457</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>1.0014</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20240305</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0053</td>\n",
       "      <td>0.9618</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>1.0491</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>1.0067</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>1.1002</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.9683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20240306</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>1.0271</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0026</td>\n",
       "      <td>1.0625</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0357</td>\n",
       "      <td>1.0176</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>1.0038</td>\n",
       "      <td>1.0238</td>\n",
       "      <td>1.0174</td>\n",
       "      <td>1.1003</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20240307</td>\n",
       "      <td>1.0048</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0102</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>1.0986</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9664</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0301</td>\n",
       "      <td>1.0110</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>1.0460</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9023</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.9935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20240308</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>1.0114</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>1.0217</td>\n",
       "      <td>1.0983</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>1.0062</td>\n",
       "      <td>1.0148</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0121</td>\n",
       "      <td>1.0887</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.1003</td>\n",
       "      <td>1.0250</td>\n",
       "      <td>1.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20240311</td>\n",
       "      <td>1.0087</td>\n",
       "      <td>1.0305</td>\n",
       "      <td>1.0271</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0101</td>\n",
       "      <td>1.0496</td>\n",
       "      <td>1.1012</td>\n",
       "      <td>1.0469</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0287</td>\n",
       "      <td>1.0170</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>1.0161</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>1.0128</td>\n",
       "      <td>1.0108</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>0.9929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20240312</td>\n",
       "      <td>1.0086</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>1.0022</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0275</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>1.0247</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>1.0169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0135</td>\n",
       "      <td>1.0076</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>1.0170</td>\n",
       "      <td>1.0264</td>\n",
       "      <td>1.0337</td>\n",
       "      <td>1.0558</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20240313</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>1.0132</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>0.9241</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0092</td>\n",
       "      <td>1.0106</td>\n",
       "      <td>1.0581</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>1.0028</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.0027</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>1.0043</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20240314</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>1.0021</td>\n",
       "      <td>1.0998</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0069</td>\n",
       "      <td>1.0821</td>\n",
       "      <td>0.9852</td>\n",
       "      <td>1.0084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>1.0342</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>1.0136</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20240315</td>\n",
       "      <td>1.0362</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>1.0434</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>1.0046</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>1.0166</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0157</td>\n",
       "      <td>1.0211</td>\n",
       "      <td>1.0312</td>\n",
       "      <td>1.0090</td>\n",
       "      <td>1.0419</td>\n",
       "      <td>1.0117</td>\n",
       "      <td>1.0107</td>\n",
       "      <td>1.0244</td>\n",
       "      <td>1.0244</td>\n",
       "      <td>1.0289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20240318</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>1.0109</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>1.0082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0317</td>\n",
       "      <td>1.0310</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>1.0053</td>\n",
       "      <td>1.1003</td>\n",
       "      <td>1.0140</td>\n",
       "      <td>1.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20240319</td>\n",
       "      <td>0.9867</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>1.0320</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0363</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>1.0388</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>1.1003</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20240320</td>\n",
       "      <td>1.0048</td>\n",
       "      <td>1.0032</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>1.0122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.0201</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>1.0112</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>1.0092</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>1.0124</td>\n",
       "      <td>0.9814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20240321</td>\n",
       "      <td>1.0019</td>\n",
       "      <td>1.0043</td>\n",
       "      <td>0.9854</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9443</td>\n",
       "      <td>1.0133</td>\n",
       "      <td>1.0065</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>1.0183</td>\n",
       "      <td>0.9237</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20240322</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>1.0223</td>\n",
       "      <td>1.0038</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>1.0092</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20240325</td>\n",
       "      <td>1.0039</td>\n",
       "      <td>1.0086</td>\n",
       "      <td>1.0027</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.9869</td>\n",
       "      <td>0.9351</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.9220</td>\n",
       "      <td>1.0022</td>\n",
       "      <td>0.9560</td>\n",
       "      <td>0.9654</td>\n",
       "      <td>1.0053</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>0.9564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20240326</td>\n",
       "      <td>1.0192</td>\n",
       "      <td>1.0128</td>\n",
       "      <td>1.0137</td>\n",
       "      <td>-100.0000</td>\n",
       "      <td>1.0233</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>1.0122</td>\n",
       "      <td>1.0086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>1.0030</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>1.0155</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>1.0549</td>\n",
       "      <td>1.0364</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    trade_date  000001.SZ  000002.SZ  000004.SZ  000005.SZ  000006.SZ  \\\n",
       "0     20240206     1.0320     1.0359     0.9051     0.9506     1.0420   \n",
       "1     20240207     0.9928     1.0011     0.9859     0.9481     1.0161   \n",
       "2     20240208     1.0062     1.0567     1.0559     0.9452     1.0714   \n",
       "3     20240219     1.0145     0.9781     1.0800     0.9565     0.9852   \n",
       "4     20240220     1.0010     1.0061     1.0144     1.0455     1.0025   \n",
       "5     20240221     1.0998     1.0313     1.0218     1.0435     1.0100   \n",
       "6     20240222     1.0093     0.9941     1.0455     1.0556     0.9975   \n",
       "7     20240223     0.9954     1.0049     1.0586     1.0526     1.0099   \n",
       "8     20240226     0.9705     0.9784     1.0335     1.0500     0.9902   \n",
       "9     20240227     0.9972     1.0080     1.0998     1.0476     1.0248   \n",
       "10    20240228     0.9990     0.9871     0.8997     1.0455     0.9661   \n",
       "11    20240229     1.0095     1.0111     1.0246     1.0543     1.0175   \n",
       "12    20240301     0.9906     0.9851     1.0232     0.9485     1.0222   \n",
       "13    20240304     0.9847     0.9535     1.0242     0.9457     0.9663   \n",
       "14    20240305     1.0097     1.0053     0.9618     0.9540     0.9875   \n",
       "15    20240306     0.9904     0.9842     1.1000  -100.0000     0.9949   \n",
       "16    20240307     1.0048     0.9871     0.9459  -100.0000     1.0102   \n",
       "17    20240308     1.0000     0.9967     1.0114  -100.0000     0.9950   \n",
       "18    20240311     1.0087     1.0305     1.0271  -100.0000     1.0101   \n",
       "19    20240312     1.0086     1.0571     1.0022  -100.0000     1.0275   \n",
       "20    20240313     0.9782     0.9690     1.0132  -100.0000     0.9732   \n",
       "21    20240314     0.9903     1.0021     1.0998  -100.0000     1.0000   \n",
       "22    20240315     1.0362     0.9753     1.0434  -100.0000     1.0025   \n",
       "23    20240318     0.9943     0.9968     0.9842  -100.0000     1.0025   \n",
       "24    20240319     0.9867     0.9883     0.9718  -100.0000     0.9876   \n",
       "25    20240320     1.0048     1.0032     0.9947  -100.0000     1.0050   \n",
       "26    20240321     1.0019     1.0043     0.9854  -100.0000     1.0025   \n",
       "27    20240322     0.9895     0.9851     0.9785  -100.0000     0.9850   \n",
       "28    20240325     1.0039     1.0086     1.0027  -100.0000     0.9822   \n",
       "29    20240326     1.0192     1.0128     1.0137  -100.0000     1.0233   \n",
       "\n",
       "    000007.SZ  000008.SZ  000009.SZ  000010.SZ  ...  603320.SH  603321.SH  \\\n",
       "0      0.9950     1.0412     1.1000     0.9610  ...     0.9091     0.9156   \n",
       "1      0.9799     0.9901     1.0991     0.9036  ...     0.9000     0.9136   \n",
       "2      1.0308     1.0350     0.9942     1.0955  ...     1.1000     1.1008   \n",
       "3      1.0149     1.0145     0.9950     1.0359  ...     1.0999     1.0534   \n",
       "4      1.0490     0.9952     1.0000     1.0396  ...     1.0306     1.0127   \n",
       "5      1.0023     1.0096     0.9858     1.0429  ...     1.0248     1.0233   \n",
       "6      0.9837     1.0047     1.0102     1.0137  ...     1.0502     1.0385   \n",
       "7      0.9763     1.0142     0.9992     1.0450  ...     1.0524     1.0589   \n",
       "8      1.0267     1.0047     0.9941     1.0216  ...     1.0315     1.0795   \n",
       "9      0.9953     1.0093     1.0101     1.0127  ...     1.0347     1.0103   \n",
       "10     0.9501     0.9679     0.9757     0.9250  ...     0.9001     0.9242   \n",
       "11     0.9950     1.0190     1.0206     1.0586  ...     1.0155     1.0268   \n",
       "12     0.9874     1.0047     1.0101     0.9830  ...     1.0036     0.9923   \n",
       "13     0.9847     0.9954     0.9750     0.9913  ...     0.9920     0.9876   \n",
       "14     1.0491     0.9907     0.9770     0.9782  ...     0.9820     0.9781   \n",
       "15     1.0271     1.0000     1.0026     1.0625  ...     1.0357     1.0176   \n",
       "16     0.9928     1.0986     0.9843     0.9664  ...     1.0301     1.0110   \n",
       "17     1.0217     1.0983     1.0000     0.9913  ...     0.9888     1.0062   \n",
       "18     1.0496     1.1012     1.0469     1.0351  ...     1.0287     1.0170   \n",
       "19     0.9865     1.0247     0.9873     1.0169  ...     1.0135     1.0076   \n",
       "20     0.9932     0.9241     0.9803     0.9958  ...     1.0092     1.0106   \n",
       "21     1.0069     1.0821     0.9852     1.0084  ...     0.9992     0.9925   \n",
       "22     1.0046     0.9483     1.0160     1.0166  ...     1.0157     1.0211   \n",
       "23     0.9955     1.0109     0.9974     1.0082  ...     1.0317     1.0310   \n",
       "24     1.0320     0.9820     0.9904     0.9919  ...     1.0363     0.9957   \n",
       "25     1.0000     0.9927     0.9965     1.0122  ...     0.9817     1.0201   \n",
       "26     0.9934     0.9631     0.9902     1.0040  ...     0.9977     0.9972   \n",
       "27     1.0223     1.0038     0.9812     0.9799  ...     0.9860     0.9816   \n",
       "28     0.9869     0.9351     0.9708     0.9549  ...     0.9732     0.9683   \n",
       "29     0.9890     0.9959     1.0122     1.0086  ...     0.9984     1.0030   \n",
       "\n",
       "    603322.SH  603323.SH  603324.SH  603325.SH  603326.SH  603327.SH  \\\n",
       "0      1.0424     1.0339     1.0262     1.0645     0.9214     1.0366   \n",
       "1      0.9500     1.0211     0.9627     0.9671     0.9197     0.9729   \n",
       "2      1.0954     0.9954     1.0999     1.0887     1.0727     1.0967   \n",
       "3      1.1001     1.0138     1.0440     1.0000     1.0610     1.0639   \n",
       "4      1.0618     1.0045     1.0289     1.0154     1.0176     1.0062   \n",
       "5      0.9799     1.0226     1.0052     1.0249     1.0565     1.0010   \n",
       "6      1.0865     1.0133     1.0189     1.0016     1.0193     1.0237   \n",
       "7      1.0221     0.9956     1.0253     1.0271     1.0641     1.0241   \n",
       "8      1.0000     0.9781     1.0107     1.0133     1.0014     1.0186   \n",
       "9      1.0341     1.0022     1.0460     1.0200     1.0274     1.0308   \n",
       "10     1.0039     0.9888     0.9082     0.9510     0.9015     0.9738   \n",
       "11     1.0589     1.0136     1.0561     1.0302     1.0355     1.0998   \n",
       "12     1.0075     1.0000     1.0191     1.0400     1.0057     1.1003   \n",
       "13     0.9843     1.0000     0.9833     0.9948     1.0014     1.0999   \n",
       "14     0.9750     1.0067     0.9701     0.9660     0.9745     1.1002   \n",
       "15     0.9901     0.9933     1.0038     1.0238     1.0174     1.1003   \n",
       "16     0.9503     1.0000     0.9971     1.0460     0.9986     0.9023   \n",
       "17     1.0148     1.0000     1.0121     1.0887     1.0057     1.1003   \n",
       "18     0.9980     0.9978     1.0161     0.9988     1.0128     1.0108   \n",
       "19     0.9758     0.9978     1.0170     1.0264     1.0337     1.0558   \n",
       "20     1.0581     0.9888     1.0028     0.9855     1.0027     0.9472   \n",
       "21     0.9750     1.0045     1.0342     0.9660     1.0136     0.9240   \n",
       "22     1.0312     1.0090     1.0419     1.0117     1.0107     1.0244   \n",
       "23     0.9903     1.0045     0.9897     1.0486     1.0053     1.1003   \n",
       "24     1.0388     0.9911     0.9791     0.9965     0.9974     1.1003   \n",
       "25     1.0999     1.0112     0.9954     0.9872     1.0092     1.0999   \n",
       "26     0.9443     1.0133     1.0065     0.9663     1.0183     0.9237   \n",
       "27     0.9704     0.9890     0.9928     0.9931     0.9769     1.0092   \n",
       "28     0.9220     1.0022     0.9560     0.9654     1.0053     0.9444   \n",
       "29     0.9310     1.0155     1.0040     0.9846     1.0549     1.0364   \n",
       "\n",
       "    603328.SH  603329.SH  \n",
       "0      1.0486     0.9616  \n",
       "1      1.0232     1.0999  \n",
       "2      1.0923     1.0387  \n",
       "3      1.0096     0.9962  \n",
       "4      1.0047     1.0130  \n",
       "5      1.0000     1.0098  \n",
       "6      1.0220     1.0007  \n",
       "7      1.0231     1.0149  \n",
       "8      1.0120     1.0140  \n",
       "9      1.0461     1.0355  \n",
       "10     0.9332     0.9762  \n",
       "11     1.0472     1.0308  \n",
       "12     1.0131     0.9868  \n",
       "13     1.0000     0.9796  \n",
       "14     0.9900     0.9683  \n",
       "15     1.0000     1.0275  \n",
       "16     0.9841     0.9935  \n",
       "17     1.0250     1.0211  \n",
       "18     1.0057     0.9929  \n",
       "19     1.0057     1.0014  \n",
       "20     1.0043     0.9864  \n",
       "21     0.9859     0.9804  \n",
       "22     1.0244     1.0289  \n",
       "23     1.0140     1.0122  \n",
       "24     1.0000     1.0356  \n",
       "25     1.0124     0.9814  \n",
       "26     1.0000     0.9951  \n",
       "27     0.9945     0.9676  \n",
       "28     0.9684     0.9564  \n",
       "29     0.9986     0.9954  \n",
       "\n",
       "[30 rows x 4097 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "datadir = os.path.join('data', dataset)\n",
    "\n",
    "# meta数据\n",
    "meta = {}\n",
    "with open(os.path.join(datadir, 'meta.pkl'), 'r') as f:\n",
    "    meta = json.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    meta_vocab_size = 4096\n",
    "def decode(id):\n",
    "    return meta['itos'][str(id)]\n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "pd_train_data = pd.read_csv(os.path.join(datadir, 'train.csv')).iloc[1:,:meta_vocab_size+1]\n",
    "pd_val_data = pd.read_csv(os.path.join(datadir, 'val.csv')).iloc[:,:meta_vocab_size+1]\n",
    "pd_val_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape is torch.Size([266, 20])\n"
     ]
    }
   ],
   "source": [
    "def trans_frame_to_id(dataframe):\n",
    "    train_data = dataframe.iloc[:, 1:]\n",
    "    # 对所有行，都取前10个最大的\n",
    "    def top_n(row, n):\n",
    "        # return row.nlargest(n).values\n",
    "        return row.nlargest(n).index.tolist()\n",
    "\n",
    "    n = 20\n",
    "    data_top_10 = train_data.apply(top_n, axis=1, n=n)\n",
    "\n",
    "    # 将结果转换为 [266, 10] 的形状\n",
    "    data_transformed = pd.DataFrame(data_top_10.tolist(), index=train_data.index)\n",
    "\n",
    "    def to_id(row):\n",
    "        return encode(row)\n",
    "    \n",
    "    data_transformed = data_transformed.apply(to_id, axis=1)\n",
    "    data_transformed = torch.stack([torch.tensor(row) for row in data_transformed])\n",
    "    return data_transformed\n",
    "\n",
    "train_data = trans_frame_to_id(pd_train_data)\n",
    "val_data = trans_frame_to_id(pd_val_data)\n",
    "\n",
    "print(f'train.shape is {train_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2184, 3509, 2807, 2366, 1955],\n",
       "         [2495, 2599, 2717,  722, 2110],\n",
       "         [1761, 1521, 3525, 2231, 1660],\n",
       "         [3525, 1883, 1783, 2423,  782],\n",
       "         [2411, 2523,  681, 2742, 2502],\n",
       "         [3060, 2750, 2024,  789, 2633],\n",
       "         [1840, 1955, 2097, 1566, 1728],\n",
       "         [1983, 2694, 2554, 1850, 2638],\n",
       "         [2083, 2197, 2747, 1631, 3026],\n",
       "         [1613, 1702, 1781, 2764, 2654],\n",
       "         [1603,  153,  141, 2418, 1698],\n",
       "         [ 163, 2366,  730, 2262, 2234]]),\n",
       " tensor([[2573, 1477, 2366, 1700, 1632],\n",
       "         [2245, 2207, 1901, 2231, 2184],\n",
       "         [2526, 2207, 2200, 2797, 2789],\n",
       "         [1783, 2184, 2632, 2071, 2046],\n",
       "         [1946, 2011, 3952, 2633, 1115],\n",
       "         [1811, 1675, 2658,  321, 2418],\n",
       "         [1695, 2082, 1714, 2005, 2772],\n",
       "         [2573, 1534, 2621, 1781, 2599],\n",
       "         [1889, 1318, 2665, 1676,  723],\n",
       "         [1850, 1761, 2245, 2717,  157],\n",
       "         [ 535,  768, 2519, 1889, 2753],\n",
       "         [1625, 1771, 1598, 2183, 1946]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 写个python程序，从pandas数据中获取一批训练数据：\n",
    "# 1. 读取pandas数据A，格式是：[trade_date, label1, label2, label3，。。。]，date的样例有：20230104，label*是数字，样例有1.0399、0.9943\n",
    "# 2. X是生成指定格式的shape = [shape, ]\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(33)\n",
    "\n",
    "def print_data_info(split, index_data, index_id):\n",
    "    data = pd_train_data if split == 'train' else pd_val_data\n",
    "    data = pd_train_data\n",
    "\n",
    "    print(f'{split}, code is {decode(index_id)}, date is {data.iloc[index_data, 0]},price_chg is {data.iloc[index_data, (index_id+1)*2]}, aount_chg is {data.iloc[index_data, (index_id+1)*2-1]}')\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    data = val_data\n",
    "\n",
    "    indices = torch.randint(len(data)-1-block_size, (batch_size, ))\n",
    "\n",
    "    # (batch, block)\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    x = x.gather(2, torch.randint(x.shape[2], (x.shape[0], x.shape[1], 1))).squeeze(-1)\n",
    "\n",
    "    # (batch, block)\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in indices])\n",
    "    y = y.gather(2, torch.randint(y.shape[2], (y.shape[0], y.shape[1], 1))).squeeze(-1)\n",
    "\n",
    "    # index = indices[0].item()\n",
    "    # print(f'index is {index}, raw_data is {data[index:index+block_size+1]}')\n",
    "    # print(f'get train_data x is {x[0]}, y is {y[0]}')\n",
    "    # print(f'get train_data_src x is {[decode(i.item()) for i in x[0]]}, y is {y[0]}')\n",
    "\n",
    "    return x, y\n",
    "\n",
    "get_batch('val')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "config is GPTConfig(block_size=5, vocab_size=4096, n_layer=8, n_head=8, n_embd=512, dropout=0, bias=False)\n",
      "number of parameters: 27.27M\n",
      "num decayed parameter tensors: 34, with 27,265,536 parameters\n",
      "num non-decayed parameter tensors: 17, with 8,704 parameters\n",
      "using fused AdamW: False\n",
      "X is tensor([[3769, 2844, 2243, 2490, 2708],\n",
      "        [3512,  851, 1575,  912, 1199],\n",
      "        [1907, 1370, 1769, 2014, 1825],\n",
      "        [2228,  933, 2011, 3378, 2794],\n",
      "        [1969, 1844, 3870, 1164,   33],\n",
      "        [2958, 1892, 3068,  479, 3092],\n",
      "        [2573,  933, 1208, 1823, 2012],\n",
      "        [1597, 2004, 1788, 2684,  211],\n",
      "        [1024,  992,  424,  925,  213],\n",
      "        [3031, 3297,  577, 2248, 2745],\n",
      "        [ 706, 3160, 2246,  272,  129],\n",
      "        [ 464, 2949, 1216,  778, 1286]])\n",
      "Y is tensor([[ 179, 3741, 2490, 1286, 3020],\n",
      "        [ 851,  690, 2229, 1502, 2400],\n",
      "        [2997, 1800, 3330,  465, 1821],\n",
      "        [1062, 2730, 1633, 3466, 2127],\n",
      "        [2774,  265,  722, 2747, 1323],\n",
      "        [3746,  494, 1299, 2795, 2656],\n",
      "        [4088, 1299, 1583, 2615, 2277],\n",
      "        [1563, 2987, 3000,  186, 1857],\n",
      "        [4012,  712, 1901, 3453,  847],\n",
      "        [ 240, 3317, 3653, 1074,  211],\n",
      "        [1971, 1937,  236, 3972,  712],\n",
      "        [3729, 3735,  871, 2421, 1853]])\n",
      "torch.Size([12, 5, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ren/miniconda3/envs/nanogpt/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=meta_vocab_size, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "\n",
    "# 遍历并打印模型中的所有权重参数\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: {param.shape}, {param}\")\n",
    "\n",
    "print(f'X is {X}')\n",
    "print(f'Y is {Y}')\n",
    "logits, loss = model(X, Y)\n",
    "print(logits.shape)\n",
    "# print(f'logits is {logits}, shape is {logits.shape}')\n",
    "# print(f'loss is {loss}, loss.shape is {loss.shape}')\n",
    "\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.3249, val loss 8.3654\n",
      "iter 0: loss 8.4224, time 280.24ms, mfu -100.00%\n",
      "iter 1: loss 8.4369, time 152.26ms, mfu -100.00%\n",
      "iter 2: loss 8.2479, time 149.11ms, mfu -100.00%\n",
      "iter 3: loss 8.3589, time 150.68ms, mfu -100.00%\n",
      "iter 4: loss 8.0871, time 147.20ms, mfu -100.00%\n",
      "iter 5: loss 8.1896, time 145.44ms, mfu 0.02%\n",
      "iter 6: loss 8.1562, time 128.98ms, mfu 0.02%\n",
      "iter 7: loss 8.5244, time 132.94ms, mfu 0.02%\n",
      "iter 8: loss 8.1771, time 150.04ms, mfu 0.02%\n",
      "iter 9: loss 8.1805, time 153.81ms, mfu 0.02%\n",
      "iter 10: loss 8.5046, time 158.50ms, mfu 0.02%\n",
      "iter 11: loss 8.0154, time 150.91ms, mfu 0.02%\n",
      "iter 12: loss 8.8403, time 152.58ms, mfu 0.02%\n",
      "iter 13: loss 8.0768, time 161.37ms, mfu 0.02%\n",
      "iter 14: loss 8.1246, time 157.39ms, mfu 0.02%\n",
      "iter 15: loss 8.1934, time 154.82ms, mfu 0.02%\n",
      "iter 16: loss 8.3130, time 154.00ms, mfu 0.02%\n",
      "iter 17: loss 8.4961, time 155.85ms, mfu 0.02%\n",
      "iter 18: loss 8.3684, time 155.46ms, mfu 0.02%\n",
      "iter 19: loss 8.2341, time 153.21ms, mfu 0.02%\n",
      "iter 20: loss 8.1879, time 135.79ms, mfu 0.02%\n",
      "iter 21: loss 8.4348, time 121.82ms, mfu 0.02%\n",
      "iter 22: loss 8.3114, time 130.91ms, mfu 0.02%\n",
      "iter 23: loss 8.0528, time 218.76ms, mfu 0.02%\n",
      "iter 24: loss 7.9245, time 156.84ms, mfu 0.02%\n",
      "iter 25: loss 7.9771, time 153.57ms, mfu 0.02%\n",
      "iter 26: loss 7.9163, time 149.00ms, mfu 0.02%\n",
      "iter 27: loss 8.2246, time 157.14ms, mfu 0.02%\n",
      "iter 28: loss 8.3106, time 143.62ms, mfu 0.02%\n",
      "iter 29: loss 8.5341, time 128.06ms, mfu 0.02%\n",
      "iter 30: loss 8.0720, time 120.51ms, mfu 0.02%\n",
      "iter 31: loss 7.8772, time 134.87ms, mfu 0.02%\n",
      "iter 32: loss 8.2589, time 118.62ms, mfu 0.02%\n",
      "iter 33: loss 8.3225, time 126.92ms, mfu 0.02%\n",
      "iter 34: loss 8.1553, time 121.17ms, mfu 0.02%\n",
      "iter 35: loss 8.4576, time 133.14ms, mfu 0.02%\n",
      "iter 36: loss 8.0586, time 151.42ms, mfu 0.02%\n",
      "iter 37: loss 8.3647, time 166.24ms, mfu 0.02%\n",
      "iter 38: loss 8.0207, time 141.40ms, mfu 0.02%\n",
      "iter 39: loss 8.1386, time 150.16ms, mfu 0.02%\n",
      "iter 40: loss 7.9754, time 152.19ms, mfu 0.02%\n",
      "iter 41: loss 8.0671, time 152.68ms, mfu 0.02%\n",
      "iter 42: loss 8.4813, time 156.85ms, mfu 0.02%\n",
      "iter 43: loss 8.2068, time 143.87ms, mfu 0.02%\n",
      "iter 44: loss 8.2354, time 125.18ms, mfu 0.02%\n",
      "iter 45: loss 8.1110, time 124.25ms, mfu 0.02%\n",
      "iter 46: loss 7.7107, time 152.34ms, mfu 0.02%\n",
      "iter 47: loss 7.6710, time 150.45ms, mfu 0.02%\n",
      "iter 48: loss 8.2667, time 152.96ms, mfu 0.02%\n",
      "iter 49: loss 8.0700, time 140.88ms, mfu 0.02%\n",
      "iter 50: loss 8.0298, time 127.57ms, mfu 0.02%\n",
      "iter 51: loss 8.2738, time 125.82ms, mfu 0.02%\n",
      "iter 52: loss 7.7512, time 148.95ms, mfu 0.02%\n",
      "iter 53: loss 7.9145, time 148.48ms, mfu 0.02%\n",
      "iter 54: loss 8.1191, time 147.67ms, mfu 0.02%\n",
      "iter 55: loss 7.9151, time 146.05ms, mfu 0.02%\n",
      "iter 56: loss 8.3550, time 146.75ms, mfu 0.02%\n",
      "iter 57: loss 8.1339, time 138.18ms, mfu 0.02%\n",
      "iter 58: loss 8.1229, time 118.48ms, mfu 0.02%\n",
      "iter 59: loss 7.7309, time 122.62ms, mfu 0.02%\n",
      "iter 60: loss 7.9460, time 120.20ms, mfu 0.02%\n",
      "iter 61: loss 8.2954, time 141.48ms, mfu 0.02%\n",
      "iter 62: loss 8.2268, time 117.73ms, mfu 0.02%\n",
      "iter 63: loss 7.8740, time 127.03ms, mfu 0.02%\n",
      "iter 64: loss 7.7170, time 152.22ms, mfu 0.02%\n",
      "iter 65: loss 8.3237, time 149.49ms, mfu 0.02%\n",
      "iter 66: loss 8.0444, time 136.23ms, mfu 0.02%\n",
      "iter 67: loss 7.8733, time 120.82ms, mfu 0.02%\n",
      "iter 68: loss 7.7992, time 116.85ms, mfu 0.02%\n",
      "iter 69: loss 7.9332, time 120.83ms, mfu 0.02%\n",
      "iter 70: loss 8.0998, time 147.65ms, mfu 0.02%\n",
      "iter 71: loss 7.7868, time 145.52ms, mfu 0.02%\n",
      "iter 72: loss 7.8335, time 150.25ms, mfu 0.02%\n",
      "iter 73: loss 7.5720, time 150.20ms, mfu 0.02%\n",
      "iter 74: loss 7.8892, time 152.22ms, mfu 0.02%\n",
      "iter 75: loss 7.9387, time 149.23ms, mfu 0.02%\n",
      "iter 76: loss 7.9237, time 148.09ms, mfu 0.02%\n",
      "iter 77: loss 8.0317, time 149.10ms, mfu 0.02%\n",
      "iter 78: loss 7.6360, time 151.66ms, mfu 0.02%\n",
      "iter 79: loss 7.8456, time 150.14ms, mfu 0.02%\n",
      "iter 80: loss 7.8502, time 147.17ms, mfu 0.02%\n",
      "iter 81: loss 8.3585, time 130.01ms, mfu 0.02%\n",
      "iter 82: loss 7.7097, time 125.31ms, mfu 0.02%\n",
      "iter 83: loss 8.1100, time 128.01ms, mfu 0.02%\n",
      "iter 84: loss 7.6391, time 119.63ms, mfu 0.02%\n",
      "iter 85: loss 7.9213, time 122.52ms, mfu 0.02%\n",
      "iter 86: loss 7.7645, time 119.39ms, mfu 0.02%\n",
      "iter 87: loss 7.9791, time 124.86ms, mfu 0.02%\n",
      "iter 88: loss 7.8889, time 120.66ms, mfu 0.02%\n",
      "iter 89: loss 7.9133, time 121.41ms, mfu 0.02%\n",
      "iter 90: loss 7.7892, time 118.74ms, mfu 0.02%\n",
      "iter 91: loss 8.1049, time 124.44ms, mfu 0.02%\n",
      "iter 92: loss 8.0777, time 119.41ms, mfu 0.02%\n",
      "iter 93: loss 8.1013, time 123.42ms, mfu 0.02%\n",
      "iter 94: loss 7.8301, time 117.08ms, mfu 0.03%\n",
      "iter 95: loss 7.8718, time 124.94ms, mfu 0.03%\n",
      "iter 96: loss 7.7274, time 119.38ms, mfu 0.03%\n",
      "iter 97: loss 7.7694, time 120.40ms, mfu 0.03%\n",
      "iter 98: loss 7.9152, time 118.61ms, mfu 0.03%\n",
      "iter 99: loss 7.9604, time 125.05ms, mfu 0.03%\n",
      "step 100: train loss 7.8804, val loss 8.4663\n",
      "iter 100: loss 7.3900, time 196.44ms, mfu 0.02%\n",
      "iter 101: loss 7.7655, time 118.30ms, mfu 0.02%\n",
      "iter 102: loss 7.8624, time 121.71ms, mfu 0.02%\n",
      "iter 103: loss 7.8541, time 120.96ms, mfu 0.02%\n",
      "iter 104: loss 7.7620, time 118.34ms, mfu 0.03%\n",
      "iter 105: loss 7.6765, time 120.84ms, mfu 0.03%\n",
      "iter 106: loss 8.1324, time 116.48ms, mfu 0.03%\n",
      "iter 107: loss 7.8580, time 117.13ms, mfu 0.03%\n",
      "iter 108: loss 7.7828, time 120.24ms, mfu 0.03%\n",
      "iter 109: loss 7.7311, time 121.68ms, mfu 0.03%\n",
      "iter 110: loss 7.6488, time 120.43ms, mfu 0.03%\n",
      "iter 111: loss 7.9567, time 122.75ms, mfu 0.03%\n",
      "iter 112: loss 7.7061, time 122.94ms, mfu 0.03%\n",
      "iter 113: loss 7.7205, time 131.41ms, mfu 0.03%\n",
      "iter 114: loss 7.6484, time 135.77ms, mfu 0.03%\n",
      "iter 115: loss 8.0757, time 143.22ms, mfu 0.02%\n",
      "iter 116: loss 7.7338, time 151.19ms, mfu 0.02%\n",
      "iter 117: loss 7.7347, time 150.46ms, mfu 0.02%\n",
      "iter 118: loss 7.7244, time 151.03ms, mfu 0.02%\n",
      "iter 119: loss 7.7143, time 152.22ms, mfu 0.02%\n",
      "iter 120: loss 7.6978, time 154.15ms, mfu 0.02%\n",
      "iter 121: loss 7.8795, time 154.91ms, mfu 0.02%\n",
      "iter 122: loss 7.5019, time 152.33ms, mfu 0.02%\n",
      "iter 123: loss 7.8403, time 154.87ms, mfu 0.02%\n",
      "iter 124: loss 7.9805, time 155.92ms, mfu 0.02%\n",
      "iter 125: loss 8.0167, time 159.32ms, mfu 0.02%\n",
      "iter 126: loss 8.1014, time 157.48ms, mfu 0.02%\n",
      "iter 127: loss 7.8560, time 158.38ms, mfu 0.02%\n",
      "iter 128: loss 7.9098, time 154.45ms, mfu 0.02%\n",
      "iter 129: loss 7.9171, time 227.00ms, mfu 0.02%\n",
      "iter 130: loss 7.6833, time 191.32ms, mfu 0.02%\n",
      "iter 131: loss 7.7448, time 159.37ms, mfu 0.02%\n",
      "iter 132: loss 7.5671, time 149.34ms, mfu 0.02%\n",
      "iter 133: loss 7.9581, time 151.34ms, mfu 0.02%\n",
      "iter 134: loss 7.6927, time 148.77ms, mfu 0.02%\n",
      "iter 135: loss 7.7657, time 157.00ms, mfu 0.02%\n",
      "iter 136: loss 7.9300, time 147.20ms, mfu 0.02%\n",
      "iter 137: loss 7.5612, time 152.48ms, mfu 0.02%\n",
      "iter 138: loss 7.6385, time 146.91ms, mfu 0.02%\n",
      "iter 139: loss 7.6690, time 153.59ms, mfu 0.02%\n",
      "iter 140: loss 7.7081, time 153.10ms, mfu 0.02%\n",
      "iter 141: loss 7.6684, time 155.21ms, mfu 0.02%\n",
      "iter 142: loss 7.7395, time 148.89ms, mfu 0.02%\n",
      "iter 143: loss 7.6147, time 152.10ms, mfu 0.02%\n",
      "iter 144: loss 7.7751, time 155.36ms, mfu 0.02%\n",
      "iter 145: loss 7.7735, time 159.14ms, mfu 0.02%\n",
      "iter 146: loss 7.7568, time 144.60ms, mfu 0.02%\n",
      "iter 147: loss 7.7392, time 156.90ms, mfu 0.02%\n",
      "iter 148: loss 7.7457, time 147.77ms, mfu 0.02%\n",
      "iter 149: loss 7.7603, time 176.58ms, mfu 0.02%\n",
      "iter 150: loss 7.8855, time 206.64ms, mfu 0.02%\n",
      "iter 151: loss 7.7187, time 157.07ms, mfu 0.02%\n",
      "iter 152: loss 7.6430, time 160.20ms, mfu 0.02%\n",
      "iter 153: loss 7.9546, time 149.19ms, mfu 0.02%\n",
      "iter 154: loss 7.7136, time 162.78ms, mfu 0.02%\n",
      "iter 155: loss 7.6919, time 158.24ms, mfu 0.02%\n",
      "iter 156: loss 7.7163, time 155.12ms, mfu 0.02%\n",
      "iter 157: loss 7.4312, time 157.83ms, mfu 0.02%\n",
      "iter 158: loss 7.7056, time 156.33ms, mfu 0.02%\n",
      "iter 159: loss 7.7794, time 150.35ms, mfu 0.02%\n",
      "iter 160: loss 7.8822, time 151.48ms, mfu 0.02%\n",
      "iter 161: loss 7.7771, time 149.54ms, mfu 0.02%\n",
      "iter 162: loss 7.7111, time 155.39ms, mfu 0.02%\n",
      "iter 163: loss 7.7618, time 148.47ms, mfu 0.02%\n",
      "iter 164: loss 7.8415, time 152.48ms, mfu 0.02%\n",
      "iter 165: loss 7.6317, time 155.45ms, mfu 0.02%\n",
      "iter 166: loss 7.9579, time 204.73ms, mfu 0.02%\n",
      "iter 167: loss 7.6862, time 194.97ms, mfu 0.02%\n",
      "iter 168: loss 7.3502, time 175.28ms, mfu 0.02%\n",
      "iter 169: loss 7.8181, time 147.86ms, mfu 0.02%\n",
      "iter 170: loss 7.8470, time 161.85ms, mfu 0.02%\n",
      "iter 171: loss 7.6908, time 197.27ms, mfu 0.02%\n",
      "iter 172: loss 7.6203, time 184.71ms, mfu 0.02%\n",
      "iter 173: loss 7.4970, time 139.84ms, mfu 0.02%\n",
      "iter 174: loss 7.7322, time 152.75ms, mfu 0.02%\n",
      "iter 175: loss 7.5478, time 146.11ms, mfu 0.02%\n",
      "iter 176: loss 7.5546, time 152.15ms, mfu 0.02%\n",
      "iter 177: loss 7.5846, time 146.93ms, mfu 0.02%\n",
      "iter 178: loss 7.7426, time 149.31ms, mfu 0.02%\n",
      "iter 179: loss 7.4311, time 200.56ms, mfu 0.02%\n",
      "iter 180: loss 7.4785, time 197.39ms, mfu 0.02%\n",
      "iter 181: loss 7.5858, time 195.41ms, mfu 0.02%\n",
      "iter 182: loss 7.5289, time 197.24ms, mfu 0.02%\n",
      "iter 183: loss 7.5756, time 155.80ms, mfu 0.02%\n",
      "iter 184: loss 7.6360, time 154.60ms, mfu 0.02%\n",
      "iter 185: loss 7.5928, time 184.87ms, mfu 0.02%\n",
      "iter 186: loss 7.6526, time 198.78ms, mfu 0.02%\n",
      "iter 187: loss 7.9064, time 197.37ms, mfu 0.02%\n",
      "iter 188: loss 7.7459, time 205.59ms, mfu 0.02%\n",
      "iter 189: loss 7.5609, time 194.50ms, mfu 0.02%\n",
      "iter 190: loss 7.7472, time 168.51ms, mfu 0.02%\n",
      "iter 191: loss 7.5008, time 167.82ms, mfu 0.02%\n",
      "iter 192: loss 7.6366, time 159.71ms, mfu 0.02%\n",
      "iter 193: loss 7.5211, time 147.97ms, mfu 0.02%\n",
      "iter 194: loss 7.8317, time 144.27ms, mfu 0.02%\n",
      "iter 195: loss 7.5998, time 145.55ms, mfu 0.02%\n",
      "iter 196: loss 7.6470, time 144.13ms, mfu 0.02%\n",
      "iter 197: loss 7.7013, time 163.88ms, mfu 0.02%\n",
      "iter 198: loss 7.5617, time 240.90ms, mfu 0.02%\n",
      "iter 199: loss 7.5841, time 147.84ms, mfu 0.02%\n",
      "step 200: train loss 7.4919, val loss 9.0316\n",
      "iter 200: loss 7.5613, time 279.53ms, mfu 0.02%\n",
      "iter 201: loss 7.6217, time 153.17ms, mfu 0.02%\n",
      "iter 202: loss 7.5610, time 151.66ms, mfu 0.02%\n",
      "iter 203: loss 7.4603, time 150.42ms, mfu 0.02%\n",
      "iter 204: loss 7.2909, time 147.89ms, mfu 0.02%\n",
      "iter 205: loss 7.5522, time 150.69ms, mfu 0.02%\n",
      "iter 206: loss 7.7781, time 147.01ms, mfu 0.02%\n",
      "iter 207: loss 7.7633, time 164.20ms, mfu 0.02%\n",
      "iter 208: loss 7.5527, time 169.10ms, mfu 0.02%\n",
      "iter 209: loss 7.3600, time 185.60ms, mfu 0.02%\n",
      "iter 210: loss 7.3955, time 165.08ms, mfu 0.02%\n",
      "iter 211: loss 7.6310, time 163.94ms, mfu 0.02%\n",
      "iter 212: loss 7.6300, time 150.78ms, mfu 0.02%\n",
      "iter 213: loss 7.7185, time 149.18ms, mfu 0.02%\n",
      "iter 214: loss 7.6024, time 144.13ms, mfu 0.02%\n",
      "iter 215: loss 7.5008, time 155.50ms, mfu 0.02%\n",
      "iter 216: loss 7.6806, time 145.88ms, mfu 0.02%\n",
      "iter 217: loss 7.5719, time 146.65ms, mfu 0.02%\n",
      "iter 218: loss 7.6438, time 151.24ms, mfu 0.02%\n",
      "iter 219: loss 7.7534, time 152.48ms, mfu 0.02%\n",
      "iter 220: loss 7.4193, time 168.29ms, mfu 0.02%\n",
      "iter 221: loss 7.5848, time 169.43ms, mfu 0.02%\n",
      "iter 222: loss 7.4482, time 144.83ms, mfu 0.02%\n",
      "iter 223: loss 7.7324, time 149.33ms, mfu 0.02%\n",
      "iter 224: loss 7.5083, time 145.52ms, mfu 0.02%\n",
      "iter 225: loss 7.6479, time 141.52ms, mfu 0.02%\n",
      "iter 226: loss 7.5956, time 142.93ms, mfu 0.02%\n",
      "iter 227: loss 7.3266, time 145.67ms, mfu 0.02%\n",
      "iter 228: loss 7.5045, time 142.31ms, mfu 0.02%\n",
      "iter 229: loss 7.8379, time 154.23ms, mfu 0.02%\n",
      "iter 230: loss 7.8458, time 154.55ms, mfu 0.02%\n",
      "iter 231: loss 7.4340, time 154.64ms, mfu 0.02%\n",
      "iter 232: loss 7.7176, time 200.05ms, mfu 0.02%\n",
      "iter 233: loss 7.5810, time 200.33ms, mfu 0.02%\n",
      "iter 234: loss 7.4230, time 196.64ms, mfu 0.02%\n",
      "iter 235: loss 7.5775, time 202.52ms, mfu 0.02%\n",
      "iter 236: loss 7.8969, time 194.20ms, mfu 0.02%\n",
      "iter 237: loss 7.5276, time 194.39ms, mfu 0.02%\n",
      "iter 238: loss 7.2352, time 183.87ms, mfu 0.02%\n",
      "iter 239: loss 7.6229, time 184.15ms, mfu 0.02%\n",
      "iter 240: loss 7.3742, time 184.70ms, mfu 0.02%\n",
      "iter 241: loss 7.6797, time 145.59ms, mfu 0.02%\n",
      "iter 242: loss 7.5021, time 158.47ms, mfu 0.02%\n",
      "iter 243: loss 7.3969, time 164.00ms, mfu 0.02%\n",
      "iter 244: loss 7.8031, time 162.48ms, mfu 0.02%\n",
      "iter 245: loss 7.6462, time 169.35ms, mfu 0.02%\n",
      "iter 246: loss 7.5238, time 184.79ms, mfu 0.02%\n",
      "iter 247: loss 7.7307, time 170.99ms, mfu 0.02%\n",
      "iter 248: loss 7.6345, time 165.60ms, mfu 0.02%\n",
      "iter 249: loss 7.5942, time 168.94ms, mfu 0.02%\n",
      "iter 250: loss 7.4019, time 164.47ms, mfu 0.02%\n",
      "iter 251: loss 7.5609, time 197.75ms, mfu 0.02%\n",
      "iter 252: loss 7.3735, time 181.95ms, mfu 0.02%\n",
      "iter 253: loss 7.5801, time 154.24ms, mfu 0.02%\n",
      "iter 254: loss 7.4327, time 167.81ms, mfu 0.02%\n",
      "iter 255: loss 7.3562, time 166.37ms, mfu 0.02%\n",
      "iter 256: loss 7.5248, time 162.16ms, mfu 0.02%\n",
      "iter 257: loss 7.6807, time 207.48ms, mfu 0.02%\n",
      "iter 258: loss 7.5497, time 158.25ms, mfu 0.02%\n",
      "iter 259: loss 7.5863, time 162.51ms, mfu 0.02%\n",
      "iter 260: loss 7.4728, time 148.72ms, mfu 0.02%\n",
      "iter 261: loss 7.5653, time 145.17ms, mfu 0.02%\n",
      "iter 262: loss 7.6649, time 163.32ms, mfu 0.02%\n",
      "iter 263: loss 7.4481, time 151.08ms, mfu 0.02%\n",
      "iter 264: loss 7.5023, time 200.10ms, mfu 0.02%\n",
      "iter 265: loss 7.5142, time 211.72ms, mfu 0.02%\n",
      "iter 266: loss 7.4058, time 199.79ms, mfu 0.02%\n",
      "iter 267: loss 7.6940, time 232.55ms, mfu 0.02%\n",
      "iter 268: loss 7.6415, time 143.02ms, mfu 0.02%\n",
      "iter 269: loss 7.6407, time 147.68ms, mfu 0.02%\n",
      "iter 270: loss 7.3373, time 143.54ms, mfu 0.02%\n",
      "iter 271: loss 7.4687, time 145.57ms, mfu 0.02%\n",
      "iter 272: loss 7.4388, time 164.22ms, mfu 0.02%\n",
      "iter 273: loss 7.4231, time 161.40ms, mfu 0.02%\n",
      "iter 274: loss 7.5990, time 160.37ms, mfu 0.02%\n",
      "iter 275: loss 7.3599, time 150.77ms, mfu 0.02%\n",
      "iter 276: loss 7.4687, time 146.01ms, mfu 0.02%\n",
      "iter 277: loss 7.6232, time 146.02ms, mfu 0.02%\n",
      "iter 278: loss 7.3149, time 148.27ms, mfu 0.02%\n",
      "iter 279: loss 7.4746, time 152.05ms, mfu 0.02%\n",
      "iter 280: loss 7.3000, time 150.19ms, mfu 0.02%\n",
      "iter 281: loss 7.4763, time 144.57ms, mfu 0.02%\n",
      "iter 282: loss 7.5762, time 147.48ms, mfu 0.02%\n",
      "iter 283: loss 7.3634, time 148.73ms, mfu 0.02%\n",
      "iter 284: loss 7.4968, time 154.42ms, mfu 0.02%\n",
      "iter 285: loss 7.3328, time 152.98ms, mfu 0.02%\n",
      "iter 286: loss 7.4110, time 148.08ms, mfu 0.02%\n",
      "iter 287: loss 7.4176, time 156.06ms, mfu 0.02%\n",
      "iter 288: loss 7.4784, time 150.47ms, mfu 0.02%\n",
      "iter 289: loss 7.4289, time 160.80ms, mfu 0.02%\n",
      "iter 290: loss 7.5495, time 142.19ms, mfu 0.02%\n",
      "iter 291: loss 7.2645, time 144.84ms, mfu 0.02%\n",
      "iter 292: loss 7.3203, time 143.11ms, mfu 0.02%\n",
      "iter 293: loss 7.4769, time 149.77ms, mfu 0.02%\n",
      "iter 294: loss 7.5000, time 141.81ms, mfu 0.02%\n",
      "iter 295: loss 7.7073, time 142.09ms, mfu 0.02%\n",
      "iter 296: loss 7.4105, time 177.99ms, mfu 0.02%\n",
      "iter 297: loss 7.2928, time 197.95ms, mfu 0.02%\n",
      "iter 298: loss 7.7410, time 201.89ms, mfu 0.02%\n",
      "iter 299: loss 7.5897, time 158.79ms, mfu 0.02%\n",
      "step 300: train loss 7.5068, val loss 8.6918\n",
      "iter 300: loss 7.4745, time 296.08ms, mfu 0.02%\n",
      "iter 301: loss 7.4531, time 166.06ms, mfu 0.02%\n",
      "iter 302: loss 7.4562, time 159.48ms, mfu 0.02%\n",
      "iter 303: loss 7.4874, time 154.34ms, mfu 0.02%\n",
      "iter 304: loss 7.4445, time 154.26ms, mfu 0.02%\n",
      "iter 305: loss 7.7419, time 147.39ms, mfu 0.02%\n",
      "iter 306: loss 7.4688, time 146.16ms, mfu 0.02%\n",
      "iter 307: loss 7.4139, time 148.57ms, mfu 0.02%\n",
      "iter 308: loss 7.4514, time 146.05ms, mfu 0.02%\n",
      "iter 309: loss 7.4473, time 144.92ms, mfu 0.02%\n",
      "iter 310: loss 7.2909, time 146.15ms, mfu 0.02%\n",
      "iter 311: loss 7.6635, time 145.94ms, mfu 0.02%\n",
      "iter 312: loss 7.3274, time 167.95ms, mfu 0.02%\n",
      "iter 313: loss 7.2704, time 197.41ms, mfu 0.02%\n",
      "iter 314: loss 7.5838, time 160.80ms, mfu 0.02%\n",
      "iter 315: loss 7.2107, time 143.50ms, mfu 0.02%\n",
      "iter 316: loss 7.6178, time 148.07ms, mfu 0.02%\n",
      "iter 317: loss 7.5223, time 161.62ms, mfu 0.02%\n",
      "iter 318: loss 7.3946, time 166.99ms, mfu 0.02%\n",
      "iter 319: loss 7.4978, time 155.98ms, mfu 0.02%\n",
      "iter 320: loss 7.6493, time 148.38ms, mfu 0.02%\n",
      "iter 321: loss 7.4020, time 146.93ms, mfu 0.02%\n",
      "iter 322: loss 7.4551, time 155.28ms, mfu 0.02%\n",
      "iter 323: loss 7.5043, time 149.99ms, mfu 0.02%\n",
      "iter 324: loss 7.2061, time 160.07ms, mfu 0.02%\n",
      "iter 325: loss 7.3536, time 158.12ms, mfu 0.02%\n",
      "iter 326: loss 7.4530, time 162.90ms, mfu 0.02%\n",
      "iter 327: loss 7.3086, time 157.43ms, mfu 0.02%\n",
      "iter 328: loss 7.5373, time 152.45ms, mfu 0.02%\n",
      "iter 329: loss 7.3397, time 150.93ms, mfu 0.02%\n",
      "iter 330: loss 7.6615, time 165.61ms, mfu 0.02%\n",
      "iter 331: loss 7.5028, time 152.99ms, mfu 0.02%\n",
      "iter 332: loss 7.4076, time 168.95ms, mfu 0.02%\n",
      "iter 333: loss 7.3113, time 197.64ms, mfu 0.02%\n",
      "iter 334: loss 7.2688, time 199.40ms, mfu 0.02%\n",
      "iter 335: loss 7.2727, time 193.73ms, mfu 0.02%\n",
      "iter 336: loss 7.6024, time 185.83ms, mfu 0.02%\n",
      "iter 337: loss 7.4989, time 187.94ms, mfu 0.02%\n",
      "iter 338: loss 7.4150, time 180.55ms, mfu 0.02%\n",
      "iter 339: loss 7.7005, time 182.75ms, mfu 0.02%\n",
      "iter 340: loss 7.3372, time 177.39ms, mfu 0.02%\n",
      "iter 341: loss 7.4960, time 153.19ms, mfu 0.02%\n",
      "iter 342: loss 7.3691, time 139.69ms, mfu 0.02%\n",
      "iter 343: loss 7.6038, time 139.66ms, mfu 0.02%\n",
      "iter 344: loss 7.5604, time 146.57ms, mfu 0.02%\n",
      "iter 345: loss 7.4649, time 138.90ms, mfu 0.02%\n",
      "iter 346: loss 7.3311, time 143.65ms, mfu 0.02%\n",
      "iter 347: loss 7.4794, time 140.12ms, mfu 0.02%\n",
      "iter 348: loss 7.1061, time 160.13ms, mfu 0.02%\n",
      "iter 349: loss 7.4241, time 149.52ms, mfu 0.02%\n",
      "iter 350: loss 7.5214, time 151.19ms, mfu 0.02%\n",
      "iter 351: loss 7.4339, time 142.26ms, mfu 0.02%\n",
      "iter 352: loss 7.4711, time 145.73ms, mfu 0.02%\n",
      "iter 353: loss 7.5771, time 155.72ms, mfu 0.02%\n",
      "iter 354: loss 7.3606, time 145.32ms, mfu 0.02%\n",
      "iter 355: loss 7.1345, time 141.14ms, mfu 0.02%\n",
      "iter 356: loss 7.4836, time 143.97ms, mfu 0.02%\n",
      "iter 357: loss 7.4631, time 146.66ms, mfu 0.02%\n",
      "iter 358: loss 7.4663, time 149.52ms, mfu 0.02%\n",
      "iter 359: loss 7.6822, time 140.44ms, mfu 0.02%\n",
      "iter 360: loss 7.4285, time 144.73ms, mfu 0.02%\n",
      "iter 361: loss 7.4472, time 140.87ms, mfu 0.02%\n",
      "iter 362: loss 7.5042, time 141.17ms, mfu 0.02%\n",
      "iter 363: loss 7.7151, time 142.08ms, mfu 0.02%\n",
      "iter 364: loss 7.4318, time 153.10ms, mfu 0.02%\n",
      "iter 365: loss 7.4081, time 143.44ms, mfu 0.02%\n",
      "iter 366: loss 7.4139, time 142.38ms, mfu 0.02%\n",
      "iter 367: loss 7.4354, time 145.88ms, mfu 0.02%\n",
      "iter 368: loss 7.3882, time 146.03ms, mfu 0.02%\n",
      "iter 369: loss 7.2346, time 142.54ms, mfu 0.02%\n",
      "iter 370: loss 7.8288, time 146.47ms, mfu 0.02%\n",
      "iter 371: loss 7.5152, time 140.75ms, mfu 0.02%\n",
      "iter 372: loss 7.4044, time 145.23ms, mfu 0.02%\n",
      "iter 373: loss 7.4735, time 144.56ms, mfu 0.02%\n",
      "iter 374: loss 7.3848, time 145.39ms, mfu 0.02%\n",
      "iter 375: loss 7.6123, time 150.46ms, mfu 0.02%\n",
      "iter 376: loss 7.6686, time 147.36ms, mfu 0.02%\n",
      "iter 377: loss 7.4200, time 148.42ms, mfu 0.02%\n",
      "iter 378: loss 7.3143, time 144.76ms, mfu 0.02%\n",
      "iter 379: loss 7.6019, time 143.80ms, mfu 0.02%\n",
      "iter 380: loss 7.4679, time 150.21ms, mfu 0.02%\n",
      "iter 381: loss 7.5227, time 144.04ms, mfu 0.02%\n",
      "iter 382: loss 7.5121, time 142.31ms, mfu 0.02%\n",
      "iter 383: loss 7.6802, time 141.39ms, mfu 0.02%\n",
      "iter 384: loss 7.4773, time 170.11ms, mfu 0.02%\n",
      "iter 385: loss 7.3372, time 195.12ms, mfu 0.02%\n",
      "iter 386: loss 7.3817, time 195.82ms, mfu 0.02%\n",
      "iter 387: loss 7.5630, time 193.95ms, mfu 0.02%\n",
      "iter 388: loss 7.3847, time 148.83ms, mfu 0.02%\n",
      "iter 389: loss 7.2642, time 139.56ms, mfu 0.02%\n",
      "iter 390: loss 7.4344, time 163.50ms, mfu 0.02%\n",
      "iter 391: loss 7.5102, time 161.19ms, mfu 0.02%\n",
      "iter 392: loss 7.2932, time 161.69ms, mfu 0.02%\n",
      "iter 393: loss 7.4472, time 152.03ms, mfu 0.02%\n",
      "iter 394: loss 7.6803, time 166.80ms, mfu 0.02%\n",
      "iter 395: loss 7.5083, time 147.88ms, mfu 0.02%\n",
      "iter 396: loss 7.3321, time 176.28ms, mfu 0.02%\n",
      "iter 397: loss 7.5951, time 197.37ms, mfu 0.02%\n",
      "iter 398: loss 7.4623, time 187.32ms, mfu 0.02%\n",
      "iter 399: loss 7.5492, time 144.99ms, mfu 0.02%\n",
      "step 400: train loss 7.2888, val loss 8.6878\n",
      "iter 400: loss 7.4545, time 265.12ms, mfu 0.02%\n",
      "iter 401: loss 7.3566, time 148.60ms, mfu 0.02%\n",
      "iter 402: loss 7.5634, time 159.82ms, mfu 0.02%\n",
      "iter 403: loss 7.4029, time 156.73ms, mfu 0.02%\n",
      "iter 404: loss 7.5049, time 149.92ms, mfu 0.02%\n",
      "iter 405: loss 7.5168, time 153.43ms, mfu 0.02%\n",
      "iter 406: loss 7.4527, time 139.61ms, mfu 0.02%\n",
      "iter 407: loss 7.6701, time 150.37ms, mfu 0.02%\n",
      "iter 408: loss 7.5734, time 143.49ms, mfu 0.02%\n",
      "iter 409: loss 7.4368, time 148.38ms, mfu 0.02%\n",
      "iter 410: loss 7.2455, time 145.33ms, mfu 0.02%\n",
      "iter 411: loss 7.4639, time 144.41ms, mfu 0.02%\n",
      "iter 412: loss 7.5731, time 140.21ms, mfu 0.02%\n",
      "iter 413: loss 7.3621, time 141.33ms, mfu 0.02%\n",
      "iter 414: loss 7.2811, time 142.08ms, mfu 0.02%\n",
      "iter 415: loss 7.5345, time 149.43ms, mfu 0.02%\n",
      "iter 416: loss 7.5119, time 137.75ms, mfu 0.02%\n",
      "iter 417: loss 7.4563, time 157.31ms, mfu 0.02%\n",
      "iter 418: loss 7.4806, time 152.29ms, mfu 0.02%\n",
      "iter 419: loss 7.4809, time 154.86ms, mfu 0.02%\n",
      "iter 420: loss 7.5878, time 148.37ms, mfu 0.02%\n",
      "iter 421: loss 7.5352, time 157.03ms, mfu 0.02%\n",
      "iter 422: loss 7.5769, time 149.65ms, mfu 0.02%\n",
      "iter 423: loss 7.5749, time 155.55ms, mfu 0.02%\n",
      "iter 424: loss 7.5503, time 153.62ms, mfu 0.02%\n",
      "iter 425: loss 7.3705, time 150.21ms, mfu 0.02%\n",
      "iter 426: loss 7.4941, time 138.15ms, mfu 0.02%\n",
      "iter 427: loss 7.6504, time 155.69ms, mfu 0.02%\n",
      "iter 428: loss 7.5429, time 156.72ms, mfu 0.02%\n",
      "iter 429: loss 7.3621, time 149.10ms, mfu 0.02%\n",
      "iter 430: loss 7.6774, time 143.68ms, mfu 0.02%\n",
      "iter 431: loss 7.4562, time 145.35ms, mfu 0.02%\n",
      "iter 432: loss 7.3363, time 145.20ms, mfu 0.02%\n",
      "iter 433: loss 7.3997, time 147.07ms, mfu 0.02%\n",
      "iter 434: loss 7.7273, time 144.49ms, mfu 0.02%\n",
      "iter 435: loss 7.4839, time 146.57ms, mfu 0.02%\n",
      "iter 436: loss 7.3025, time 141.16ms, mfu 0.02%\n",
      "iter 437: loss 7.4481, time 150.27ms, mfu 0.02%\n",
      "iter 438: loss 7.5699, time 137.70ms, mfu 0.02%\n",
      "iter 439: loss 7.5275, time 149.75ms, mfu 0.02%\n",
      "iter 440: loss 7.3678, time 140.88ms, mfu 0.02%\n",
      "iter 441: loss 7.3481, time 144.22ms, mfu 0.02%\n",
      "iter 442: loss 7.4448, time 264.03ms, mfu 0.02%\n",
      "iter 443: loss 7.3449, time 146.73ms, mfu 0.02%\n",
      "iter 444: loss 7.3759, time 149.32ms, mfu 0.02%\n",
      "iter 445: loss 7.2955, time 141.17ms, mfu 0.02%\n",
      "iter 446: loss 7.3092, time 145.68ms, mfu 0.02%\n",
      "iter 447: loss 7.4520, time 141.44ms, mfu 0.02%\n",
      "iter 448: loss 7.2176, time 147.05ms, mfu 0.02%\n",
      "iter 449: loss 7.2511, time 141.68ms, mfu 0.02%\n",
      "iter 450: loss 7.3966, time 148.85ms, mfu 0.02%\n",
      "iter 451: loss 7.4158, time 144.59ms, mfu 0.02%\n",
      "iter 452: loss 7.4501, time 144.49ms, mfu 0.02%\n",
      "iter 453: loss 7.3925, time 145.01ms, mfu 0.02%\n",
      "iter 454: loss 7.4486, time 145.13ms, mfu 0.02%\n",
      "iter 455: loss 7.3523, time 138.97ms, mfu 0.02%\n",
      "iter 456: loss 7.6762, time 153.46ms, mfu 0.02%\n",
      "iter 457: loss 7.6886, time 143.27ms, mfu 0.02%\n",
      "iter 458: loss 7.4648, time 156.08ms, mfu 0.02%\n",
      "iter 459: loss 7.5704, time 149.26ms, mfu 0.02%\n",
      "iter 460: loss 7.4113, time 146.10ms, mfu 0.02%\n",
      "iter 461: loss 7.4395, time 146.92ms, mfu 0.02%\n",
      "iter 462: loss 7.4969, time 142.16ms, mfu 0.02%\n",
      "iter 463: loss 7.3251, time 143.06ms, mfu 0.02%\n",
      "iter 464: loss 7.5467, time 148.66ms, mfu 0.02%\n",
      "iter 465: loss 7.3662, time 140.60ms, mfu 0.02%\n",
      "iter 466: loss 7.4988, time 143.78ms, mfu 0.02%\n",
      "iter 467: loss 7.3975, time 143.13ms, mfu 0.02%\n",
      "iter 468: loss 7.5809, time 143.50ms, mfu 0.02%\n",
      "iter 469: loss 7.6352, time 144.90ms, mfu 0.02%\n",
      "iter 470: loss 7.4192, time 144.28ms, mfu 0.02%\n",
      "iter 471: loss 7.5053, time 146.25ms, mfu 0.02%\n",
      "iter 472: loss 7.4582, time 142.32ms, mfu 0.02%\n",
      "iter 473: loss 7.4455, time 137.74ms, mfu 0.02%\n",
      "iter 474: loss 7.4195, time 147.08ms, mfu 0.02%\n",
      "iter 475: loss 7.4257, time 149.49ms, mfu 0.02%\n",
      "iter 476: loss 7.5444, time 155.89ms, mfu 0.02%\n",
      "iter 477: loss 7.4530, time 164.80ms, mfu 0.02%\n",
      "iter 478: loss 7.4404, time 159.33ms, mfu 0.02%\n",
      "iter 479: loss 7.6240, time 202.64ms, mfu 0.02%\n",
      "iter 480: loss 7.2761, time 195.86ms, mfu 0.02%\n",
      "iter 481: loss 7.5939, time 256.51ms, mfu 0.02%\n",
      "iter 482: loss 7.4497, time 204.77ms, mfu 0.02%\n",
      "iter 483: loss 7.3620, time 207.31ms, mfu 0.02%\n",
      "iter 484: loss 7.5252, time 196.80ms, mfu 0.02%\n",
      "iter 485: loss 7.6293, time 189.13ms, mfu 0.02%\n",
      "iter 486: loss 7.4646, time 188.44ms, mfu 0.02%\n",
      "iter 487: loss 7.5410, time 184.24ms, mfu 0.02%\n",
      "iter 488: loss 7.4153, time 142.82ms, mfu 0.02%\n",
      "iter 489: loss 7.7086, time 142.94ms, mfu 0.02%\n",
      "iter 490: loss 7.3386, time 139.37ms, mfu 0.02%\n",
      "iter 491: loss 7.5282, time 139.28ms, mfu 0.02%\n",
      "iter 492: loss 7.5928, time 139.88ms, mfu 0.02%\n",
      "iter 493: loss 7.4032, time 144.57ms, mfu 0.02%\n",
      "iter 494: loss 7.3789, time 141.29ms, mfu 0.02%\n",
      "iter 495: loss 7.5330, time 153.31ms, mfu 0.02%\n",
      "iter 496: loss 7.4503, time 160.96ms, mfu 0.02%\n",
      "iter 497: loss 7.3152, time 154.06ms, mfu 0.02%\n",
      "iter 498: loss 7.3667, time 148.18ms, mfu 0.02%\n",
      "iter 499: loss 7.3768, time 151.57ms, mfu 0.02%\n",
      "step 500: train loss 7.4586, val loss 8.8477\n",
      "iter 500: loss 7.2956, time 289.05ms, mfu 0.02%\n",
      "iter 501: loss 7.4580, time 157.33ms, mfu 0.02%\n",
      "iter 502: loss 7.5512, time 153.90ms, mfu 0.02%\n",
      "iter 503: loss 7.4582, time 157.43ms, mfu 0.02%\n",
      "iter 504: loss 7.5422, time 177.79ms, mfu 0.02%\n",
      "iter 505: loss 7.3341, time 198.83ms, mfu 0.02%\n",
      "iter 506: loss 7.3183, time 183.87ms, mfu 0.02%\n",
      "iter 507: loss 7.4739, time 134.98ms, mfu 0.02%\n",
      "iter 508: loss 7.3925, time 148.17ms, mfu 0.02%\n",
      "iter 509: loss 7.4813, time 159.45ms, mfu 0.02%\n",
      "iter 510: loss 7.4979, time 148.04ms, mfu 0.02%\n",
      "iter 511: loss 7.6452, time 142.67ms, mfu 0.02%\n",
      "iter 512: loss 7.3812, time 153.24ms, mfu 0.02%\n",
      "iter 513: loss 7.5481, time 148.54ms, mfu 0.02%\n",
      "iter 514: loss 7.4109, time 147.83ms, mfu 0.02%\n",
      "iter 515: loss 7.3412, time 145.41ms, mfu 0.02%\n",
      "iter 516: loss 7.3174, time 146.37ms, mfu 0.02%\n",
      "iter 517: loss 7.5779, time 147.45ms, mfu 0.02%\n",
      "iter 518: loss 7.4696, time 148.96ms, mfu 0.02%\n",
      "iter 519: loss 7.3561, time 146.60ms, mfu 0.02%\n",
      "iter 520: loss 7.2278, time 151.15ms, mfu 0.02%\n",
      "iter 521: loss 7.2210, time 146.02ms, mfu 0.02%\n",
      "iter 522: loss 7.5103, time 143.20ms, mfu 0.02%\n",
      "iter 523: loss 7.4815, time 143.71ms, mfu 0.02%\n",
      "iter 524: loss 7.2191, time 144.28ms, mfu 0.02%\n",
      "iter 525: loss 7.2136, time 150.96ms, mfu 0.02%\n",
      "iter 526: loss 7.5520, time 165.38ms, mfu 0.02%\n",
      "iter 527: loss 7.2973, time 188.11ms, mfu 0.02%\n",
      "iter 528: loss 7.5711, time 187.82ms, mfu 0.02%\n",
      "iter 529: loss 7.2731, time 146.34ms, mfu 0.02%\n",
      "iter 530: loss 7.5186, time 145.03ms, mfu 0.02%\n",
      "iter 531: loss 7.5211, time 169.34ms, mfu 0.02%\n",
      "iter 532: loss 7.3087, time 144.10ms, mfu 0.02%\n",
      "iter 533: loss 7.4651, time 136.38ms, mfu 0.02%\n",
      "iter 534: loss 7.1853, time 143.18ms, mfu 0.02%\n",
      "iter 535: loss 7.5529, time 200.22ms, mfu 0.02%\n",
      "iter 536: loss 7.4936, time 144.54ms, mfu 0.02%\n",
      "iter 537: loss 7.2637, time 146.32ms, mfu 0.02%\n",
      "iter 538: loss 7.4302, time 155.44ms, mfu 0.02%\n",
      "iter 539: loss 7.4950, time 157.57ms, mfu 0.02%\n",
      "iter 540: loss 7.3929, time 189.75ms, mfu 0.02%\n",
      "iter 541: loss 7.2836, time 187.23ms, mfu 0.02%\n",
      "iter 542: loss 7.2488, time 186.81ms, mfu 0.02%\n",
      "iter 543: loss 7.1232, time 179.93ms, mfu 0.02%\n",
      "iter 544: loss 7.3095, time 188.48ms, mfu 0.02%\n",
      "iter 545: loss 7.2783, time 187.89ms, mfu 0.02%\n",
      "iter 546: loss 7.4717, time 187.72ms, mfu 0.02%\n",
      "iter 547: loss 7.2908, time 187.95ms, mfu 0.02%\n",
      "iter 548: loss 7.5334, time 186.96ms, mfu 0.02%\n",
      "iter 549: loss 7.3541, time 183.10ms, mfu 0.02%\n",
      "iter 550: loss 7.4925, time 199.17ms, mfu 0.02%\n",
      "iter 551: loss 7.5211, time 192.16ms, mfu 0.02%\n",
      "iter 552: loss 7.6172, time 191.80ms, mfu 0.02%\n",
      "iter 553: loss 7.3273, time 188.99ms, mfu 0.02%\n",
      "iter 554: loss 7.3514, time 193.10ms, mfu 0.02%\n",
      "iter 555: loss 7.5094, time 186.09ms, mfu 0.02%\n",
      "iter 556: loss 7.2382, time 180.32ms, mfu 0.02%\n",
      "iter 557: loss 7.3816, time 180.95ms, mfu 0.02%\n",
      "iter 558: loss 7.3454, time 180.42ms, mfu 0.02%\n",
      "iter 559: loss 7.8735, time 183.36ms, mfu 0.02%\n",
      "iter 560: loss 7.6368, time 184.83ms, mfu 0.02%\n",
      "iter 561: loss 7.4608, time 182.26ms, mfu 0.02%\n",
      "iter 562: loss 7.3664, time 180.36ms, mfu 0.02%\n",
      "iter 563: loss 7.2988, time 180.50ms, mfu 0.02%\n",
      "iter 564: loss 7.3797, time 183.20ms, mfu 0.02%\n",
      "iter 565: loss 7.3090, time 199.08ms, mfu 0.02%\n",
      "iter 566: loss 7.5416, time 207.93ms, mfu 0.02%\n",
      "iter 567: loss 7.2103, time 197.79ms, mfu 0.02%\n",
      "iter 568: loss 7.2654, time 190.98ms, mfu 0.02%\n",
      "iter 569: loss 7.4477, time 190.11ms, mfu 0.02%\n",
      "iter 570: loss 7.4381, time 186.87ms, mfu 0.02%\n",
      "iter 571: loss 7.7051, time 193.13ms, mfu 0.02%\n",
      "iter 572: loss 7.4452, time 181.69ms, mfu 0.02%\n",
      "iter 573: loss 7.2324, time 179.31ms, mfu 0.02%\n",
      "iter 574: loss 7.5715, time 184.07ms, mfu 0.02%\n",
      "iter 575: loss 7.4304, time 181.55ms, mfu 0.02%\n",
      "iter 576: loss 7.4513, time 186.79ms, mfu 0.02%\n",
      "iter 577: loss 7.4171, time 184.60ms, mfu 0.02%\n",
      "iter 578: loss 7.4076, time 181.16ms, mfu 0.02%\n",
      "iter 579: loss 7.2830, time 182.94ms, mfu 0.02%\n",
      "iter 580: loss 7.2820, time 179.13ms, mfu 0.02%\n",
      "iter 581: loss 7.4462, time 177.84ms, mfu 0.02%\n",
      "iter 582: loss 7.6646, time 182.76ms, mfu 0.02%\n",
      "iter 583: loss 7.4979, time 186.10ms, mfu 0.02%\n",
      "iter 584: loss 7.6332, time 182.36ms, mfu 0.02%\n",
      "iter 585: loss 7.6358, time 181.29ms, mfu 0.02%\n",
      "iter 586: loss 7.3911, time 185.68ms, mfu 0.02%\n",
      "iter 587: loss 7.4395, time 181.63ms, mfu 0.02%\n",
      "iter 588: loss 7.3480, time 178.97ms, mfu 0.02%\n",
      "iter 589: loss 7.3692, time 179.28ms, mfu 0.02%\n",
      "iter 590: loss 7.4595, time 183.03ms, mfu 0.02%\n",
      "iter 591: loss 7.4288, time 181.92ms, mfu 0.02%\n",
      "iter 592: loss 7.4722, time 182.93ms, mfu 0.02%\n",
      "iter 593: loss 7.4099, time 178.00ms, mfu 0.02%\n",
      "iter 594: loss 7.4505, time 174.54ms, mfu 0.02%\n",
      "iter 595: loss 7.4737, time 178.62ms, mfu 0.02%\n",
      "iter 596: loss 7.3556, time 185.21ms, mfu 0.02%\n",
      "iter 597: loss 7.4257, time 184.22ms, mfu 0.02%\n",
      "iter 598: loss 7.2251, time 183.31ms, mfu 0.02%\n",
      "iter 599: loss 7.5960, time 183.32ms, mfu 0.02%\n",
      "step 600: train loss 7.4514, val loss 9.1599\n",
      "iter 600: loss 7.3370, time 356.88ms, mfu 0.02%\n",
      "iter 601: loss 7.5533, time 186.87ms, mfu 0.02%\n",
      "iter 602: loss 7.4336, time 187.59ms, mfu 0.02%\n",
      "iter 603: loss 7.6040, time 187.35ms, mfu 0.02%\n",
      "iter 604: loss 7.2807, time 196.66ms, mfu 0.02%\n",
      "iter 605: loss 7.4508, time 193.17ms, mfu 0.02%\n",
      "iter 606: loss 7.5030, time 183.17ms, mfu 0.02%\n",
      "iter 607: loss 7.5553, time 183.56ms, mfu 0.02%\n",
      "iter 608: loss 7.2601, time 179.57ms, mfu 0.02%\n",
      "iter 609: loss 7.4610, time 188.42ms, mfu 0.02%\n",
      "iter 610: loss 7.3622, time 173.80ms, mfu 0.02%\n",
      "iter 611: loss 7.3757, time 141.71ms, mfu 0.02%\n",
      "iter 612: loss 7.4462, time 141.77ms, mfu 0.02%\n",
      "iter 613: loss 7.5808, time 144.00ms, mfu 0.02%\n",
      "iter 614: loss 7.3904, time 139.89ms, mfu 0.02%\n",
      "iter 615: loss 7.4607, time 145.55ms, mfu 0.02%\n",
      "iter 616: loss 7.3198, time 149.14ms, mfu 0.02%\n",
      "iter 617: loss 7.2154, time 149.70ms, mfu 0.02%\n",
      "iter 618: loss 7.3424, time 138.94ms, mfu 0.02%\n",
      "iter 619: loss 7.4282, time 146.13ms, mfu 0.02%\n",
      "iter 620: loss 7.6596, time 147.16ms, mfu 0.02%\n",
      "iter 621: loss 7.4957, time 148.32ms, mfu 0.02%\n",
      "iter 622: loss 7.5137, time 141.75ms, mfu 0.02%\n",
      "iter 623: loss 7.4180, time 145.24ms, mfu 0.02%\n",
      "iter 624: loss 7.4529, time 141.22ms, mfu 0.02%\n",
      "iter 625: loss 7.3727, time 146.70ms, mfu 0.02%\n",
      "iter 626: loss 7.6305, time 146.81ms, mfu 0.02%\n",
      "iter 627: loss 7.3732, time 143.27ms, mfu 0.02%\n",
      "iter 628: loss 7.6302, time 169.79ms, mfu 0.02%\n",
      "iter 629: loss 7.6216, time 189.63ms, mfu 0.02%\n",
      "iter 630: loss 7.3926, time 164.69ms, mfu 0.02%\n",
      "iter 631: loss 7.4021, time 141.44ms, mfu 0.02%\n",
      "iter 632: loss 7.3951, time 137.56ms, mfu 0.02%\n",
      "iter 633: loss 7.5748, time 151.23ms, mfu 0.02%\n",
      "iter 634: loss 7.4316, time 144.63ms, mfu 0.02%\n",
      "iter 635: loss 7.3774, time 144.83ms, mfu 0.02%\n",
      "iter 636: loss 7.4698, time 144.56ms, mfu 0.02%\n",
      "iter 637: loss 7.3341, time 142.84ms, mfu 0.02%\n",
      "iter 638: loss 7.5026, time 145.18ms, mfu 0.02%\n",
      "iter 639: loss 7.3213, time 142.87ms, mfu 0.02%\n",
      "iter 640: loss 7.3910, time 145.86ms, mfu 0.02%\n",
      "iter 641: loss 7.3852, time 143.56ms, mfu 0.02%\n",
      "iter 642: loss 7.2220, time 140.45ms, mfu 0.02%\n",
      "iter 643: loss 7.2650, time 148.30ms, mfu 0.02%\n",
      "iter 644: loss 7.2934, time 146.32ms, mfu 0.02%\n",
      "iter 645: loss 7.1850, time 148.49ms, mfu 0.02%\n",
      "iter 646: loss 7.3261, time 143.99ms, mfu 0.02%\n",
      "iter 647: loss 7.4869, time 146.11ms, mfu 0.02%\n",
      "iter 648: loss 7.4670, time 144.84ms, mfu 0.02%\n",
      "iter 649: loss 7.5182, time 150.81ms, mfu 0.02%\n",
      "iter 650: loss 7.4240, time 146.11ms, mfu 0.02%\n",
      "iter 651: loss 7.3481, time 147.59ms, mfu 0.02%\n",
      "iter 652: loss 7.2836, time 142.14ms, mfu 0.02%\n",
      "iter 653: loss 7.2085, time 148.12ms, mfu 0.02%\n",
      "iter 654: loss 7.2609, time 139.50ms, mfu 0.02%\n",
      "iter 655: loss 7.5353, time 154.24ms, mfu 0.02%\n",
      "iter 656: loss 7.2623, time 141.11ms, mfu 0.02%\n",
      "iter 657: loss 7.2941, time 143.92ms, mfu 0.02%\n",
      "iter 658: loss 7.4347, time 141.87ms, mfu 0.02%\n",
      "iter 659: loss 7.3307, time 151.20ms, mfu 0.02%\n",
      "iter 660: loss 7.5575, time 140.38ms, mfu 0.02%\n",
      "iter 661: loss 7.2586, time 151.30ms, mfu 0.02%\n",
      "iter 662: loss 7.6191, time 140.65ms, mfu 0.02%\n",
      "iter 663: loss 7.6330, time 149.67ms, mfu 0.02%\n",
      "iter 664: loss 7.3283, time 146.13ms, mfu 0.02%\n",
      "iter 665: loss 7.4617, time 148.34ms, mfu 0.02%\n",
      "iter 666: loss 7.6298, time 138.58ms, mfu 0.02%\n",
      "iter 667: loss 7.1932, time 143.99ms, mfu 0.02%\n",
      "iter 668: loss 7.4639, time 143.67ms, mfu 0.02%\n",
      "iter 669: loss 7.2364, time 151.58ms, mfu 0.02%\n",
      "iter 670: loss 7.4118, time 149.43ms, mfu 0.02%\n",
      "iter 671: loss 7.6909, time 141.22ms, mfu 0.02%\n",
      "iter 672: loss 7.4421, time 142.88ms, mfu 0.02%\n",
      "iter 673: loss 7.4449, time 144.54ms, mfu 0.02%\n",
      "iter 674: loss 7.4875, time 141.62ms, mfu 0.02%\n",
      "iter 675: loss 7.3350, time 145.44ms, mfu 0.02%\n",
      "iter 676: loss 7.4076, time 142.29ms, mfu 0.02%\n",
      "iter 677: loss 7.1291, time 145.33ms, mfu 0.02%\n",
      "iter 678: loss 7.4503, time 144.63ms, mfu 0.02%\n",
      "iter 679: loss 7.3109, time 150.29ms, mfu 0.02%\n",
      "iter 680: loss 7.5681, time 137.19ms, mfu 0.02%\n",
      "iter 681: loss 7.2588, time 154.54ms, mfu 0.02%\n",
      "iter 682: loss 7.3819, time 139.90ms, mfu 0.02%\n",
      "iter 683: loss 7.2986, time 151.32ms, mfu 0.02%\n",
      "iter 684: loss 7.3546, time 137.55ms, mfu 0.02%\n",
      "iter 685: loss 7.3904, time 143.28ms, mfu 0.02%\n",
      "iter 686: loss 7.5728, time 146.18ms, mfu 0.02%\n",
      "iter 687: loss 7.5192, time 156.46ms, mfu 0.02%\n",
      "iter 688: loss 7.4877, time 190.02ms, mfu 0.02%\n",
      "iter 689: loss 7.2896, time 208.98ms, mfu 0.02%\n",
      "iter 690: loss 7.2249, time 277.26ms, mfu 0.02%\n",
      "iter 691: loss 7.3836, time 193.79ms, mfu 0.02%\n",
      "iter 692: loss 7.3148, time 177.04ms, mfu 0.02%\n",
      "iter 693: loss 7.4753, time 139.59ms, mfu 0.02%\n",
      "iter 694: loss 7.2098, time 147.63ms, mfu 0.02%\n",
      "iter 695: loss 7.2598, time 140.75ms, mfu 0.02%\n",
      "iter 696: loss 7.4455, time 143.25ms, mfu 0.02%\n",
      "iter 697: loss 7.3419, time 147.23ms, mfu 0.02%\n",
      "iter 698: loss 7.5109, time 143.51ms, mfu 0.02%\n",
      "iter 699: loss 7.2958, time 146.28ms, mfu 0.02%\n",
      "step 700: train loss 7.4136, val loss 9.0372\n",
      "iter 700: loss 7.5483, time 269.67ms, mfu 0.02%\n",
      "iter 701: loss 7.3867, time 147.56ms, mfu 0.02%\n",
      "iter 702: loss 7.4097, time 144.19ms, mfu 0.02%\n",
      "iter 703: loss 7.4240, time 147.46ms, mfu 0.02%\n",
      "iter 704: loss 7.5262, time 143.73ms, mfu 0.02%\n",
      "iter 705: loss 7.5613, time 155.82ms, mfu 0.02%\n",
      "iter 706: loss 7.6017, time 154.68ms, mfu 0.02%\n",
      "iter 707: loss 7.6046, time 148.02ms, mfu 0.02%\n",
      "iter 708: loss 7.4683, time 144.49ms, mfu 0.02%\n",
      "iter 709: loss 7.3081, time 140.25ms, mfu 0.02%\n",
      "iter 710: loss 7.3667, time 144.20ms, mfu 0.02%\n",
      "iter 711: loss 7.4023, time 141.35ms, mfu 0.02%\n",
      "iter 712: loss 7.4648, time 141.22ms, mfu 0.02%\n",
      "iter 713: loss 7.2058, time 146.51ms, mfu 0.02%\n",
      "iter 714: loss 7.5752, time 147.04ms, mfu 0.02%\n",
      "iter 715: loss 7.4870, time 145.90ms, mfu 0.02%\n",
      "iter 716: loss 7.4261, time 145.69ms, mfu 0.02%\n",
      "iter 717: loss 7.3768, time 144.44ms, mfu 0.02%\n",
      "iter 718: loss 7.4775, time 143.19ms, mfu 0.02%\n",
      "iter 719: loss 7.3961, time 140.80ms, mfu 0.02%\n",
      "iter 720: loss 7.3694, time 145.26ms, mfu 0.02%\n",
      "iter 721: loss 7.6077, time 145.45ms, mfu 0.02%\n",
      "iter 722: loss 7.5235, time 149.64ms, mfu 0.02%\n",
      "iter 723: loss 7.3932, time 148.82ms, mfu 0.02%\n",
      "iter 724: loss 7.4004, time 145.44ms, mfu 0.02%\n",
      "iter 725: loss 7.4148, time 150.72ms, mfu 0.02%\n",
      "iter 726: loss 7.4216, time 183.03ms, mfu 0.02%\n",
      "iter 727: loss 7.4128, time 150.94ms, mfu 0.02%\n",
      "iter 728: loss 7.3787, time 147.83ms, mfu 0.02%\n",
      "iter 729: loss 7.4337, time 148.09ms, mfu 0.02%\n",
      "iter 730: loss 7.3528, time 151.32ms, mfu 0.02%\n",
      "iter 731: loss 7.2946, time 146.32ms, mfu 0.02%\n",
      "iter 732: loss 7.3731, time 141.15ms, mfu 0.02%\n",
      "iter 733: loss 7.3454, time 150.59ms, mfu 0.02%\n",
      "iter 734: loss 7.3368, time 150.08ms, mfu 0.02%\n",
      "iter 735: loss 7.4337, time 149.68ms, mfu 0.02%\n",
      "iter 736: loss 7.3144, time 183.29ms, mfu 0.02%\n",
      "iter 737: loss 7.3888, time 147.27ms, mfu 0.02%\n",
      "iter 738: loss 7.4252, time 134.79ms, mfu 0.02%\n",
      "iter 739: loss 7.4052, time 143.43ms, mfu 0.02%\n",
      "iter 740: loss 7.3556, time 138.31ms, mfu 0.02%\n",
      "iter 741: loss 7.2869, time 155.66ms, mfu 0.02%\n",
      "iter 742: loss 7.3488, time 136.68ms, mfu 0.02%\n",
      "iter 743: loss 7.5965, time 144.65ms, mfu 0.02%\n",
      "iter 744: loss 7.4387, time 140.46ms, mfu 0.02%\n",
      "iter 745: loss 7.2281, time 146.64ms, mfu 0.02%\n",
      "iter 746: loss 7.4997, time 146.24ms, mfu 0.02%\n",
      "iter 747: loss 7.5734, time 146.81ms, mfu 0.02%\n",
      "iter 748: loss 7.4200, time 157.28ms, mfu 0.02%\n",
      "iter 749: loss 7.1925, time 162.69ms, mfu 0.02%\n",
      "iter 750: loss 7.3220, time 141.37ms, mfu 0.02%\n",
      "iter 751: loss 7.5126, time 145.85ms, mfu 0.02%\n",
      "iter 752: loss 7.5823, time 144.80ms, mfu 0.02%\n",
      "iter 753: loss 7.1429, time 143.70ms, mfu 0.02%\n",
      "iter 754: loss 7.5602, time 141.32ms, mfu 0.02%\n",
      "iter 755: loss 7.5709, time 136.91ms, mfu 0.02%\n",
      "iter 756: loss 7.3289, time 145.41ms, mfu 0.02%\n",
      "iter 757: loss 7.4157, time 151.25ms, mfu 0.02%\n",
      "iter 758: loss 7.4004, time 143.80ms, mfu 0.02%\n",
      "iter 759: loss 7.4921, time 145.65ms, mfu 0.02%\n",
      "iter 760: loss 7.4747, time 143.89ms, mfu 0.02%\n",
      "iter 761: loss 7.3882, time 146.63ms, mfu 0.02%\n",
      "iter 762: loss 7.4105, time 142.46ms, mfu 0.02%\n",
      "iter 763: loss 7.3000, time 146.90ms, mfu 0.02%\n",
      "iter 764: loss 7.4192, time 140.46ms, mfu 0.02%\n",
      "iter 765: loss 7.3630, time 144.84ms, mfu 0.02%\n",
      "iter 766: loss 7.2262, time 146.16ms, mfu 0.02%\n",
      "iter 767: loss 7.3209, time 142.33ms, mfu 0.02%\n",
      "iter 768: loss 7.3260, time 142.65ms, mfu 0.02%\n",
      "iter 769: loss 7.4578, time 143.82ms, mfu 0.02%\n",
      "iter 770: loss 7.3325, time 144.27ms, mfu 0.02%\n",
      "iter 771: loss 7.5735, time 145.39ms, mfu 0.02%\n",
      "iter 772: loss 7.3526, time 148.64ms, mfu 0.02%\n",
      "iter 773: loss 7.5730, time 144.17ms, mfu 0.02%\n",
      "iter 774: loss 7.3226, time 140.08ms, mfu 0.02%\n",
      "iter 775: loss 7.5316, time 146.95ms, mfu 0.02%\n",
      "iter 776: loss 7.4554, time 146.77ms, mfu 0.02%\n",
      "iter 777: loss 7.4560, time 147.19ms, mfu 0.02%\n",
      "iter 778: loss 7.2702, time 145.66ms, mfu 0.02%\n",
      "iter 779: loss 7.2844, time 140.64ms, mfu 0.02%\n",
      "iter 780: loss 7.3084, time 143.32ms, mfu 0.02%\n",
      "iter 781: loss 7.4076, time 144.36ms, mfu 0.02%\n",
      "iter 782: loss 7.4224, time 192.44ms, mfu 0.02%\n",
      "iter 783: loss 7.4443, time 194.24ms, mfu 0.02%\n",
      "iter 784: loss 7.2519, time 183.75ms, mfu 0.02%\n",
      "iter 785: loss 7.3023, time 188.73ms, mfu 0.02%\n",
      "iter 786: loss 7.5266, time 179.06ms, mfu 0.02%\n",
      "iter 787: loss 7.5058, time 186.41ms, mfu 0.02%\n",
      "iter 788: loss 7.8955, time 183.87ms, mfu 0.02%\n",
      "iter 789: loss 7.5025, time 186.02ms, mfu 0.02%\n",
      "iter 790: loss 7.3707, time 188.61ms, mfu 0.02%\n",
      "iter 791: loss 7.5136, time 181.15ms, mfu 0.02%\n",
      "iter 792: loss 7.2662, time 181.85ms, mfu 0.02%\n",
      "iter 793: loss 7.4754, time 187.61ms, mfu 0.02%\n",
      "iter 794: loss 7.4012, time 185.25ms, mfu 0.02%\n",
      "iter 795: loss 7.3903, time 180.11ms, mfu 0.02%\n",
      "iter 796: loss 7.2697, time 185.62ms, mfu 0.02%\n",
      "iter 797: loss 7.3966, time 176.00ms, mfu 0.02%\n",
      "iter 798: loss 7.3745, time 183.02ms, mfu 0.02%\n",
      "iter 799: loss 7.4851, time 194.03ms, mfu 0.02%\n",
      "step 800: train loss 7.3728, val loss 9.2016\n",
      "iter 800: loss 7.1155, time 358.95ms, mfu 0.02%\n",
      "iter 801: loss 7.4562, time 190.21ms, mfu 0.02%\n",
      "iter 802: loss 7.2913, time 188.70ms, mfu 0.02%\n",
      "iter 803: loss 7.5130, time 186.14ms, mfu 0.02%\n",
      "iter 804: loss 7.3578, time 182.79ms, mfu 0.02%\n",
      "iter 805: loss 7.3863, time 180.70ms, mfu 0.02%\n",
      "iter 806: loss 7.3983, time 182.30ms, mfu 0.02%\n",
      "iter 807: loss 7.3997, time 185.29ms, mfu 0.02%\n",
      "iter 808: loss 7.2738, time 184.05ms, mfu 0.02%\n",
      "iter 809: loss 7.2841, time 179.33ms, mfu 0.02%\n",
      "iter 810: loss 7.5750, time 187.08ms, mfu 0.02%\n",
      "iter 811: loss 7.3378, time 183.55ms, mfu 0.02%\n",
      "iter 812: loss 7.3424, time 181.99ms, mfu 0.02%\n",
      "iter 813: loss 7.4809, time 181.16ms, mfu 0.02%\n",
      "iter 814: loss 7.4915, time 189.09ms, mfu 0.02%\n",
      "iter 815: loss 7.4387, time 185.03ms, mfu 0.02%\n",
      "iter 816: loss 7.4998, time 195.97ms, mfu 0.02%\n",
      "iter 817: loss 7.3570, time 188.09ms, mfu 0.02%\n",
      "iter 818: loss 7.2700, time 185.58ms, mfu 0.02%\n",
      "iter 819: loss 7.2958, time 183.37ms, mfu 0.02%\n",
      "iter 820: loss 7.3090, time 190.17ms, mfu 0.02%\n",
      "iter 821: loss 7.4679, time 184.21ms, mfu 0.02%\n",
      "iter 822: loss 7.4665, time 183.30ms, mfu 0.02%\n",
      "iter 823: loss 7.4269, time 181.29ms, mfu 0.02%\n",
      "iter 824: loss 7.4050, time 182.24ms, mfu 0.02%\n",
      "iter 825: loss 7.3515, time 187.64ms, mfu 0.02%\n",
      "iter 826: loss 7.3283, time 181.77ms, mfu 0.02%\n",
      "iter 827: loss 7.5921, time 179.28ms, mfu 0.02%\n",
      "iter 828: loss 7.4312, time 179.09ms, mfu 0.02%\n",
      "iter 829: loss 7.4552, time 180.95ms, mfu 0.02%\n",
      "iter 830: loss 7.2622, time 181.06ms, mfu 0.02%\n",
      "iter 831: loss 7.4439, time 181.56ms, mfu 0.02%\n",
      "iter 832: loss 7.4518, time 184.48ms, mfu 0.02%\n",
      "iter 833: loss 7.3544, time 182.84ms, mfu 0.02%\n",
      "iter 834: loss 7.5151, time 186.80ms, mfu 0.02%\n",
      "iter 835: loss 7.2325, time 183.29ms, mfu 0.02%\n",
      "iter 836: loss 7.4168, time 180.66ms, mfu 0.02%\n",
      "iter 837: loss 7.3365, time 183.62ms, mfu 0.02%\n",
      "iter 838: loss 7.2235, time 181.02ms, mfu 0.02%\n",
      "iter 839: loss 7.2483, time 184.42ms, mfu 0.02%\n",
      "iter 840: loss 7.2991, time 185.91ms, mfu 0.02%\n",
      "iter 841: loss 7.4515, time 187.57ms, mfu 0.02%\n",
      "iter 842: loss 7.3703, time 178.57ms, mfu 0.02%\n",
      "iter 843: loss 7.3578, time 145.79ms, mfu 0.02%\n",
      "iter 844: loss 7.4103, time 191.45ms, mfu 0.02%\n",
      "iter 845: loss 7.3583, time 188.47ms, mfu 0.02%\n",
      "iter 846: loss 7.4928, time 190.81ms, mfu 0.02%\n",
      "iter 847: loss 7.3197, time 188.85ms, mfu 0.02%\n",
      "iter 848: loss 7.4094, time 184.10ms, mfu 0.02%\n",
      "iter 849: loss 7.3541, time 187.66ms, mfu 0.02%\n",
      "iter 850: loss 7.5406, time 183.23ms, mfu 0.02%\n",
      "iter 851: loss 7.4252, time 141.33ms, mfu 0.02%\n",
      "iter 852: loss 7.6083, time 145.29ms, mfu 0.02%\n",
      "iter 853: loss 7.5002, time 167.23ms, mfu 0.02%\n",
      "iter 854: loss 7.4679, time 185.25ms, mfu 0.02%\n",
      "iter 855: loss 7.3914, time 188.75ms, mfu 0.02%\n",
      "iter 856: loss 7.4793, time 185.78ms, mfu 0.02%\n",
      "iter 857: loss 7.3003, time 176.29ms, mfu 0.02%\n",
      "iter 858: loss 7.5160, time 185.39ms, mfu 0.02%\n",
      "iter 859: loss 7.4626, time 183.27ms, mfu 0.02%\n",
      "iter 860: loss 7.2669, time 133.50ms, mfu 0.02%\n",
      "iter 861: loss 7.3671, time 135.91ms, mfu 0.02%\n",
      "iter 862: loss 7.4801, time 140.48ms, mfu 0.02%\n",
      "iter 863: loss 7.2258, time 136.53ms, mfu 0.02%\n",
      "iter 864: loss 7.4054, time 142.82ms, mfu 0.02%\n",
      "iter 865: loss 7.3890, time 140.06ms, mfu 0.02%\n",
      "iter 866: loss 7.2473, time 147.99ms, mfu 0.02%\n",
      "iter 867: loss 7.2989, time 144.64ms, mfu 0.02%\n",
      "iter 868: loss 7.5731, time 148.27ms, mfu 0.02%\n",
      "iter 869: loss 7.4296, time 143.01ms, mfu 0.02%\n",
      "iter 870: loss 7.3325, time 159.36ms, mfu 0.02%\n",
      "iter 871: loss 7.3749, time 146.64ms, mfu 0.02%\n",
      "iter 872: loss 7.3522, time 153.50ms, mfu 0.02%\n",
      "iter 873: loss 7.2925, time 197.59ms, mfu 0.02%\n",
      "iter 874: loss 7.2801, time 150.42ms, mfu 0.02%\n",
      "iter 875: loss 7.4485, time 146.90ms, mfu 0.02%\n",
      "iter 876: loss 7.5098, time 144.21ms, mfu 0.02%\n",
      "iter 877: loss 7.3313, time 145.85ms, mfu 0.02%\n",
      "iter 878: loss 7.4552, time 146.61ms, mfu 0.02%\n",
      "iter 879: loss 7.3571, time 138.46ms, mfu 0.02%\n",
      "iter 880: loss 7.3986, time 159.61ms, mfu 0.02%\n",
      "iter 881: loss 7.3444, time 197.66ms, mfu 0.02%\n",
      "iter 882: loss 7.3034, time 193.22ms, mfu 0.02%\n",
      "iter 883: loss 7.3108, time 189.82ms, mfu 0.02%\n",
      "iter 884: loss 7.8307, time 190.14ms, mfu 0.02%\n",
      "iter 885: loss 7.2677, time 141.55ms, mfu 0.02%\n",
      "iter 886: loss 7.3386, time 139.18ms, mfu 0.02%\n",
      "iter 887: loss 7.4980, time 145.16ms, mfu 0.02%\n",
      "iter 888: loss 7.2376, time 144.16ms, mfu 0.02%\n",
      "iter 889: loss 7.3401, time 145.25ms, mfu 0.02%\n",
      "iter 890: loss 7.4215, time 142.62ms, mfu 0.02%\n",
      "iter 891: loss 7.3436, time 148.00ms, mfu 0.02%\n",
      "iter 892: loss 7.4184, time 144.75ms, mfu 0.02%\n",
      "iter 893: loss 7.4519, time 138.92ms, mfu 0.02%\n",
      "iter 894: loss 7.3718, time 142.28ms, mfu 0.02%\n",
      "iter 895: loss 7.3340, time 145.60ms, mfu 0.02%\n",
      "iter 896: loss 7.5000, time 143.93ms, mfu 0.02%\n",
      "iter 897: loss 7.2680, time 140.48ms, mfu 0.02%\n",
      "iter 898: loss 7.3068, time 148.45ms, mfu 0.02%\n",
      "iter 899: loss 7.5247, time 143.24ms, mfu 0.02%\n",
      "step 900: train loss 7.1185, val loss 9.3051\n",
      "iter 900: loss 7.3815, time 267.94ms, mfu 0.02%\n",
      "iter 901: loss 7.2395, time 146.54ms, mfu 0.02%\n",
      "iter 902: loss 7.4196, time 143.44ms, mfu 0.02%\n",
      "iter 903: loss 7.4530, time 148.17ms, mfu 0.02%\n",
      "iter 904: loss 7.4196, time 142.30ms, mfu 0.02%\n",
      "iter 905: loss 7.4336, time 141.66ms, mfu 0.02%\n",
      "iter 906: loss 7.2941, time 144.28ms, mfu 0.02%\n",
      "iter 907: loss 7.2737, time 159.45ms, mfu 0.02%\n",
      "iter 908: loss 7.4258, time 141.74ms, mfu 0.02%\n",
      "iter 909: loss 7.4406, time 149.98ms, mfu 0.02%\n",
      "iter 910: loss 7.4274, time 140.26ms, mfu 0.02%\n",
      "iter 911: loss 7.4351, time 143.47ms, mfu 0.02%\n",
      "iter 912: loss 7.3483, time 137.50ms, mfu 0.02%\n",
      "iter 913: loss 7.3773, time 139.20ms, mfu 0.02%\n",
      "iter 914: loss 7.1645, time 149.66ms, mfu 0.02%\n",
      "iter 915: loss 7.4998, time 144.45ms, mfu 0.02%\n",
      "iter 916: loss 7.2613, time 141.33ms, mfu 0.02%\n",
      "iter 917: loss 7.4195, time 147.63ms, mfu 0.02%\n",
      "iter 918: loss 7.5069, time 144.86ms, mfu 0.02%\n",
      "iter 919: loss 7.3613, time 155.29ms, mfu 0.02%\n",
      "iter 920: loss 7.6142, time 149.82ms, mfu 0.02%\n",
      "iter 921: loss 7.3604, time 147.61ms, mfu 0.02%\n",
      "iter 922: loss 7.2678, time 148.22ms, mfu 0.02%\n",
      "iter 923: loss 7.3551, time 149.83ms, mfu 0.02%\n",
      "iter 924: loss 7.4898, time 144.71ms, mfu 0.02%\n",
      "iter 925: loss 7.3193, time 148.80ms, mfu 0.02%\n",
      "iter 926: loss 7.3679, time 144.90ms, mfu 0.02%\n",
      "iter 927: loss 7.3285, time 148.03ms, mfu 0.02%\n",
      "iter 928: loss 7.1861, time 140.94ms, mfu 0.02%\n",
      "iter 929: loss 7.2763, time 149.53ms, mfu 0.02%\n",
      "iter 930: loss 7.6378, time 148.07ms, mfu 0.02%\n",
      "iter 931: loss 7.5294, time 145.68ms, mfu 0.02%\n",
      "iter 932: loss 7.3390, time 143.42ms, mfu 0.02%\n",
      "iter 933: loss 7.3744, time 148.30ms, mfu 0.02%\n",
      "iter 934: loss 7.2833, time 146.61ms, mfu 0.02%\n",
      "iter 935: loss 7.3125, time 147.47ms, mfu 0.02%\n",
      "iter 936: loss 7.3572, time 141.97ms, mfu 0.02%\n",
      "iter 937: loss 7.3463, time 149.78ms, mfu 0.02%\n",
      "iter 938: loss 7.5009, time 142.88ms, mfu 0.02%\n",
      "iter 939: loss 7.4947, time 144.26ms, mfu 0.02%\n",
      "iter 940: loss 7.3844, time 142.12ms, mfu 0.02%\n",
      "iter 941: loss 7.3214, time 244.31ms, mfu 0.02%\n",
      "iter 942: loss 7.2290, time 172.50ms, mfu 0.02%\n",
      "iter 943: loss 7.2258, time 147.75ms, mfu 0.02%\n",
      "iter 944: loss 7.3108, time 143.22ms, mfu 0.02%\n",
      "iter 945: loss 7.4358, time 149.39ms, mfu 0.02%\n",
      "iter 946: loss 7.5211, time 144.22ms, mfu 0.02%\n",
      "iter 947: loss 7.4595, time 147.19ms, mfu 0.02%\n",
      "iter 948: loss 7.5241, time 144.14ms, mfu 0.02%\n",
      "iter 949: loss 7.4881, time 148.94ms, mfu 0.02%\n",
      "iter 950: loss 7.6829, time 142.69ms, mfu 0.02%\n",
      "iter 951: loss 7.4310, time 143.14ms, mfu 0.02%\n",
      "iter 952: loss 7.3539, time 146.47ms, mfu 0.02%\n",
      "iter 953: loss 7.2801, time 146.73ms, mfu 0.02%\n",
      "iter 954: loss 7.4095, time 148.01ms, mfu 0.02%\n",
      "iter 955: loss 7.5225, time 148.45ms, mfu 0.02%\n",
      "iter 956: loss 7.3617, time 147.59ms, mfu 0.02%\n",
      "iter 957: loss 7.4326, time 150.47ms, mfu 0.02%\n",
      "iter 958: loss 7.4467, time 150.27ms, mfu 0.02%\n",
      "iter 959: loss 7.4321, time 145.65ms, mfu 0.02%\n",
      "iter 960: loss 7.2350, time 145.43ms, mfu 0.02%\n",
      "iter 961: loss 7.5518, time 152.86ms, mfu 0.02%\n",
      "iter 962: loss 7.2999, time 145.60ms, mfu 0.02%\n",
      "iter 963: loss 7.3071, time 156.04ms, mfu 0.02%\n",
      "iter 964: loss 7.4722, time 149.38ms, mfu 0.02%\n",
      "iter 965: loss 7.4540, time 148.22ms, mfu 0.02%\n",
      "iter 966: loss 7.3732, time 143.17ms, mfu 0.02%\n",
      "iter 967: loss 7.4746, time 148.32ms, mfu 0.02%\n",
      "iter 968: loss 7.2748, time 144.34ms, mfu 0.02%\n",
      "iter 969: loss 7.4422, time 147.04ms, mfu 0.02%\n",
      "iter 970: loss 7.4559, time 149.19ms, mfu 0.02%\n",
      "iter 971: loss 7.3973, time 154.47ms, mfu 0.02%\n",
      "iter 972: loss 7.3259, time 185.03ms, mfu 0.02%\n",
      "iter 973: loss 7.2957, time 154.16ms, mfu 0.02%\n",
      "iter 974: loss 7.3228, time 158.14ms, mfu 0.02%\n",
      "iter 975: loss 7.3761, time 194.27ms, mfu 0.02%\n",
      "iter 976: loss 7.4420, time 205.61ms, mfu 0.02%\n",
      "iter 977: loss 7.2899, time 194.63ms, mfu 0.02%\n",
      "iter 978: loss 7.3707, time 171.78ms, mfu 0.02%\n",
      "iter 979: loss 7.3554, time 145.13ms, mfu 0.02%\n",
      "iter 980: loss 7.3054, time 141.13ms, mfu 0.02%\n",
      "iter 981: loss 7.4771, time 147.33ms, mfu 0.02%\n",
      "iter 982: loss 7.4583, time 152.54ms, mfu 0.02%\n",
      "iter 983: loss 7.3727, time 146.13ms, mfu 0.02%\n",
      "iter 984: loss 7.3273, time 148.26ms, mfu 0.02%\n",
      "iter 985: loss 7.6935, time 143.40ms, mfu 0.02%\n",
      "iter 986: loss 7.5153, time 143.39ms, mfu 0.02%\n",
      "iter 987: loss 7.2851, time 147.49ms, mfu 0.02%\n",
      "iter 988: loss 7.3201, time 153.88ms, mfu 0.02%\n",
      "iter 989: loss 7.3347, time 144.37ms, mfu 0.02%\n",
      "iter 990: loss 7.2551, time 149.70ms, mfu 0.02%\n",
      "iter 991: loss 7.3506, time 148.56ms, mfu 0.02%\n",
      "iter 992: loss 7.3908, time 182.64ms, mfu 0.02%\n",
      "iter 993: loss 7.5952, time 188.79ms, mfu 0.02%\n",
      "iter 994: loss 7.4329, time 141.79ms, mfu 0.02%\n",
      "iter 995: loss 7.3862, time 142.40ms, mfu 0.02%\n",
      "iter 996: loss 7.3432, time 150.31ms, mfu 0.02%\n",
      "iter 997: loss 7.1964, time 143.06ms, mfu 0.02%\n",
      "iter 998: loss 7.5180, time 145.36ms, mfu 0.02%\n",
      "iter 999: loss 7.3396, time 140.61ms, mfu 0.02%\n",
      "step 1000: train loss 7.4221, val loss 9.3193\n",
      "iter 1000: loss 7.4759, time 274.46ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training loop\n",
    "# X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "                \n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "    \n",
    "    X, Y = get_batch('train')\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
