{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out-stock'\n",
    "eval_interval = 10\n",
    "log_interval = 1\n",
    "eval_iters = 20\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'stock'\n",
    "wandb_run_name = 'mini-gpt' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'stock'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 32\n",
    "# model\n",
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 2048\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "meta_vocab_size = 1024\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 300 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3750, 1.9512, 1.7324,  ..., 1.7227, 1.0674, 1.7080],\n",
      "        [1.5439, 1.0820, 1.4580,  ..., 1.9287, 1.3320, 1.4658],\n",
      "        [1.0137, 1.0820, 1.2588,  ..., 1.3613, 1.1084, 1.8760],\n",
      "        ...,\n",
      "        [1.1025, 1.9570, 1.1562,  ..., 1.0205, 1.3867, 1.1455],\n",
      "        [1.8369, 1.5430, 1.1621,  ..., 1.6475, 1.8945, 1.8662],\n",
      "        [1.3232, 1.4258, 1.4482,  ..., 1.1123, 1.4482, 1.7236]],\n",
      "       dtype=torch.float16)\n",
      "batch is (tensor([[[1.9658, 1.5693, 1.4180,  ..., 1.8115, 1.6172, 1.8281],\n",
      "         [1.4512, 1.1787, 1.9180,  ..., 1.1953, 1.9824, 1.8711],\n",
      "         [1.3467, 1.8301, 1.7646,  ..., 1.6143, 1.1836, 1.6270],\n",
      "         ...,\n",
      "         [1.5039, 1.5156, 1.0068,  ..., 1.5703, 1.7275, 1.8740],\n",
      "         [1.1816, 1.3008, 1.9297,  ..., 1.4561, 1.0137, 1.2529],\n",
      "         [1.6836, 1.5762, 1.7451,  ..., 1.8350, 1.1289, 1.8818]],\n",
      "\n",
      "        [[1.6543, 1.7607, 1.1777,  ..., 1.4414, 1.6191, 1.8994],\n",
      "         [1.3301, 1.8496, 1.0938,  ..., 1.2178, 1.8330, 1.0947],\n",
      "         [1.4043, 1.1992, 1.8760,  ..., 1.5801, 1.0830, 1.2178],\n",
      "         ...,\n",
      "         [1.5039, 1.7334, 1.7021,  ..., 1.7754, 1.7490, 1.0498],\n",
      "         [1.1777, 1.5732, 1.8672,  ..., 1.5576, 1.0039, 1.8477],\n",
      "         [1.3691, 1.7236, 1.7695,  ..., 1.2344, 1.7197, 1.2148]],\n",
      "\n",
      "        [[1.3281, 1.4492, 1.8311,  ..., 1.4219, 1.2900, 1.0156],\n",
      "         [1.9492, 1.1934, 1.7998,  ..., 1.5918, 1.4502, 1.7539],\n",
      "         [1.0117, 1.5068, 1.7812,  ..., 1.6680, 1.3359, 1.8047],\n",
      "         ...,\n",
      "         [1.1973, 1.2773, 1.0977,  ..., 1.7930, 1.9209, 1.6523],\n",
      "         [1.8838, 1.9180, 1.2070,  ..., 1.6133, 1.1963, 1.7217],\n",
      "         [1.0420, 1.3213, 1.2568,  ..., 1.2910, 1.1094, 1.7041]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.7598, 1.9951, 1.0449,  ..., 1.4482, 1.3994, 1.2197],\n",
      "         [1.8730, 1.0635, 1.4326,  ..., 1.9590, 1.7412, 1.1172],\n",
      "         [1.9414, 1.6650, 1.0244,  ..., 1.1895, 1.5361, 1.6709],\n",
      "         ...,\n",
      "         [1.9336, 1.5762, 1.8135,  ..., 1.3213, 1.8525, 1.8350],\n",
      "         [1.1152, 1.7686, 1.5459,  ..., 1.0840, 1.3633, 1.0879],\n",
      "         [1.3467, 1.5693, 1.5869,  ..., 1.3662, 1.6553, 1.4092]],\n",
      "\n",
      "        [[1.2061, 1.9795, 1.9609,  ..., 1.9082, 1.6709, 1.9453],\n",
      "         [1.6416, 1.2734, 1.9873,  ..., 1.3037, 1.3867, 1.9521],\n",
      "         [1.4570, 1.2725, 1.8750,  ..., 1.5469, 1.1230, 1.9590],\n",
      "         ...,\n",
      "         [1.1953, 1.3721, 1.9482,  ..., 1.6504, 1.9473, 1.4355],\n",
      "         [1.2539, 1.2256, 1.4404,  ..., 1.7666, 1.9902, 1.3936],\n",
      "         [1.2617, 1.8408, 1.3076,  ..., 1.6133, 1.3545, 1.9248]],\n",
      "\n",
      "        [[1.5479, 1.1445, 1.8350,  ..., 1.4023, 1.4814, 1.8340],\n",
      "         [1.3809, 1.3496, 1.7256,  ..., 1.9209, 1.0078, 1.3213],\n",
      "         [1.5820, 1.5146, 1.5459,  ..., 1.6064, 1.4199, 1.6260],\n",
      "         ...,\n",
      "         [1.4639, 1.8691, 1.1377,  ..., 1.9707, 1.3223, 1.9434],\n",
      "         [1.5713, 1.1846, 1.4355,  ..., 1.4961, 1.8955, 1.7646],\n",
      "         [1.1035, 1.6885, 1.8955,  ..., 1.5400, 1.8193, 1.5127]]],\n",
      "       dtype=torch.float16), tensor([[  34,  868,  647,  777,  552,  611,  554,  203,   79,  698,  725,  372,\n",
      "          363,  934,  178,  192,  648,  871, 1002,  993,  153,  176,  953,  846,\n",
      "          795,  204,  242,  657,  602,  345,  120,  172],\n",
      "        [ 934,  352,   98,  172,  275,   32,  712,  200,  435,  277,   93,  223,\n",
      "          473,  534,  923,  433, 1019,  579,  936,  751,  914,  274,  618,  893,\n",
      "          187,  843,  438,  408,  369,  736,  522,  908],\n",
      "        [  29,  538,  553,   94,  870,  171,  755,  276,  596,  799,  418,  534,\n",
      "          474,  312,  836,  795,  225,  965,  949,   76,   52,  134,  506,  654,\n",
      "          566,  759,  435,  343,   67,  691,  398,   87],\n",
      "        [ 903,   54,   79,  676,  583,  557,  544, 1002,  708,  737,  596,  336,\n",
      "          432,  762,  315,  618, 1023,  164,  139,  343,  813,  589,  984,  435,\n",
      "          856,   37,  179,  316,  608,  228,  442,  443],\n",
      "        [ 912,  207,  337,   35,  669,  638,  316,   60,  626,  619,  753,  741,\n",
      "          595,  111,  278,  780,  240,   43,  238,  981,  436,   36,  501,  246,\n",
      "          155,  346,  392,  704,  586,  747,   25,  692],\n",
      "        [ 401, 1000,  364,  442,  248,  497,  986,   70,  342,  620,  409,  109,\n",
      "            5,  594,   43,  823,  569,  729,  354,  694,  137,  155,  955,  480,\n",
      "          424,  111,  673,  530,  227,  828,   83,  540],\n",
      "        [  34,  591,  394,  331,  242,  287,  266,  907,  892,  893,  126,   13,\n",
      "          248,  782,  958,  753,  216,  625,  753,  340,  306,  747,  553,  931,\n",
      "          586,  729,  671,  330,   85,  274,  893,   43],\n",
      "        [ 992,  191,  988,  167,  423,  474,  363,  681,   23,  348,   96,  723,\n",
      "          762,  859,  760,  449,  129,  653,  940,  289,  890,  718,  787,  153,\n",
      "          888,   14,   39,  884,  599,  905,  688,   92],\n",
      "        [ 744,  824,  838,  980,  404,  813,  406,  433,   36,  573,  302,  890,\n",
      "          906,  208,  841,  712,  276,  222,  887,  138,  221,   98,  467,  943,\n",
      "          349,  208,   31,  966,  973,  517,  655,   96],\n",
      "        [ 424,  580,  604,  423,  327,   90,  212,  813,  627,  292,  815,  352,\n",
      "          705,  179,  636,  655,  237,  258,  995,   76,  474,  688,  607,  620,\n",
      "          790,  679,  553,  870,  298,  512,  389,  494],\n",
      "        [ 946,  555,  719,   73,  352,  994,  680,  456,  658,  787,  656,   52,\n",
      "          754,  673,  430,  612,  700,  931,    1,  183,  492,  935,    6,   52,\n",
      "          871,  284,  147,  141,  120,  923,  774,  926],\n",
      "        [  24, 1015,  118,  575,  750,  685,   29,  341,  486,  651,  795,  489,\n",
      "          652,  665,  841,  512,  757,  634,  965,  304,  365,  773,  126, 1012,\n",
      "          968,  604,  991,  975,  199,  757,  738,  683]]))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# 生成50x32大小的数组，值范围在1到10之间\n",
    "data = np.random.rand(365, n_embd) + 1\n",
    "# 将NumPy数组转换为pandas DataFrame\n",
    "train_data = pd.DataFrame(data)\n",
    "train_data = torch.tensor(train_data.values, dtype=torch.float16)\n",
    "val_data = train_data\n",
    "print(train_data)\n",
    "torch.manual_seed(33)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    indices = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    y_indices = [torch.arange(0, len(x[i][0]) // 2) for i, j in enumerate(indices)]\n",
    "    \n",
    "    y_list = []\n",
    "    for i, j in enumerate(indices):\n",
    "        next_row = data[j + 1:j + 1 + block_size]\n",
    "        price_chg_cols = [2 * col + 1 for col in y_indices[i]]\n",
    "        \n",
    "        topk = 20\n",
    "        topk_values, topk_indices = next_row[:, price_chg_cols].topk(topk, dim=1)\n",
    "\n",
    "        random_indices = torch.randint(0, topk, (topk_indices.size(0),))\n",
    "        selected_indices = topk_indices[torch.arange(topk_indices.size(0)), random_indices]\n",
    "\n",
    "        y_list.append(selected_indices.clone().detach())\n",
    "        \n",
    "\n",
    "    y = torch.stack(y_list)\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "print(f'batch is {get_batch('train')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "number of parameters: 304.11M\n",
      "num decayed parameter tensors: 26, with 304,152,576 parameters\n",
      "num non-decayed parameter tensors: 13, with 26,624 parameters\n",
      "using fused AdamW: False\n",
      "logits is tensor([[[ 1.2725e+00,  1.0312e+00,  5.9189e-01,  ...,  1.1907e+00,\n",
      "           1.2011e-01,  3.8435e-01],\n",
      "         [ 7.6813e-01,  6.9559e-01,  1.0309e+00,  ...,  1.9059e+00,\n",
      "          -5.3035e-01, -6.5802e-01],\n",
      "         [ 2.9543e-01,  7.5959e-01,  4.0179e-01,  ...,  1.7852e+00,\n",
      "          -5.0921e-01,  8.8802e-02],\n",
      "         ...,\n",
      "         [ 1.1047e+00,  1.7412e+00, -6.0531e-01,  ...,  1.7644e+00,\n",
      "          -4.8889e-01,  9.7142e-01],\n",
      "         [-1.0298e+00, -4.3449e-01,  9.8726e-01,  ...,  4.2481e-01,\n",
      "          -4.7614e-01,  4.0978e-01],\n",
      "         [ 3.5275e-01, -3.2725e-01,  1.5238e+00,  ...,  1.6243e+00,\n",
      "           4.8180e-01, -1.2518e-01]],\n",
      "\n",
      "        [[ 9.5166e-02,  7.9497e-01,  1.5387e-01,  ...,  1.5123e+00,\n",
      "          -7.2758e-01,  1.3022e-01],\n",
      "         [-1.0282e-01,  7.2772e-01,  9.7088e-01,  ...,  1.1298e+00,\n",
      "          -1.5415e-01,  1.2141e+00],\n",
      "         [-1.1585e-01,  1.2240e+00,  1.0447e+00,  ...,  8.7283e-01,\n",
      "           5.0949e-01,  1.0778e+00],\n",
      "         ...,\n",
      "         [ 4.1570e-01,  1.8350e+00,  2.2345e+00,  ...,  9.9426e-01,\n",
      "          -8.9382e-01,  6.9885e-01],\n",
      "         [ 4.5189e-01,  1.3206e+00,  4.8305e-01,  ...,  2.2944e+00,\n",
      "          -1.2355e+00, -1.0903e+00],\n",
      "         [ 1.1340e+00,  1.2492e+00, -1.2321e+00,  ...,  5.5348e-01,\n",
      "          -5.8320e-02, -2.0235e-01]],\n",
      "\n",
      "        [[ 5.8834e-01, -1.7307e-01, -2.7230e-01,  ..., -5.0820e-01,\n",
      "          -4.8413e-01, -6.6504e-01],\n",
      "         [ 4.3153e-01,  1.4684e+00,  3.1033e-01,  ..., -8.8167e-01,\n",
      "          -1.0112e+00, -2.2449e+00],\n",
      "         [ 1.3170e-01,  2.2934e-01,  2.6172e-01,  ...,  1.2272e+00,\n",
      "          -1.4010e+00, -8.9804e-01],\n",
      "         ...,\n",
      "         [ 1.5307e-01,  2.2396e+00,  5.8873e-01,  ..., -4.7873e-01,\n",
      "          -3.1969e-01,  8.4886e-01],\n",
      "         [ 9.1939e-01,  1.4940e+00,  1.0589e+00,  ...,  2.6780e-01,\n",
      "           2.1448e-01, -9.5870e-02],\n",
      "         [ 5.8104e-01, -1.4883e-01, -3.9750e-01,  ...,  1.5328e+00,\n",
      "          -5.2070e-01,  3.0165e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5253e+00, -2.1112e-01, -1.8150e+00,  ...,  1.0296e+00,\n",
      "           4.3315e-01, -2.0516e+00],\n",
      "         [ 9.5102e-01,  1.4618e-01, -6.5450e-04,  ...,  6.7606e-01,\n",
      "           2.6649e-01, -7.9217e-01],\n",
      "         [-5.4396e-01,  1.0107e-01, -4.8127e-01,  ..., -3.1574e-02,\n",
      "          -1.4379e+00, -1.1398e+00],\n",
      "         ...,\n",
      "         [ 8.7045e-01,  2.0190e+00, -3.1903e-01,  ...,  1.0804e+00,\n",
      "           1.4568e-01, -1.2160e+00],\n",
      "         [ 2.9357e-01,  5.3944e-01,  3.9994e-02,  ...,  1.7004e+00,\n",
      "          -4.4787e-02,  6.6862e-01],\n",
      "         [-4.6312e-01, -2.4024e-01, -7.3010e-02,  ...,  3.3434e-01,\n",
      "          -6.5960e-01,  4.9232e-01]],\n",
      "\n",
      "        [[ 7.4466e-01,  1.0332e+00,  8.0807e-01,  ...,  1.3633e+00,\n",
      "          -1.1694e+00,  7.8514e-01],\n",
      "         [-8.6500e-02,  1.6265e+00,  6.2619e-01,  ...,  6.6567e-01,\n",
      "          -9.2281e-02,  4.7053e-01],\n",
      "         [-8.4614e-02,  7.4174e-01,  4.1526e-02,  ...,  8.8311e-01,\n",
      "          -1.3407e+00,  2.0869e+00],\n",
      "         ...,\n",
      "         [-3.6588e-01,  1.3839e+00, -3.6173e-01,  ...,  9.2682e-01,\n",
      "          -4.7817e-01,  2.0028e-01],\n",
      "         [ 7.0062e-01,  2.0588e+00,  2.6597e-01,  ...,  1.5683e+00,\n",
      "          -4.2538e-01,  1.0798e+00],\n",
      "         [ 1.2248e+00,  5.6499e-01, -1.2381e-01,  ...,  3.6348e-01,\n",
      "          -3.9557e-01, -8.4942e-01]],\n",
      "\n",
      "        [[ 1.5557e+00,  1.2183e+00, -1.5615e-01,  ...,  8.4929e-01,\n",
      "          -6.3885e-01, -1.7303e-01],\n",
      "         [ 7.6504e-01,  6.1614e-01,  1.3680e+00,  ...,  7.1443e-01,\n",
      "          -1.2649e+00,  1.4482e-01],\n",
      "         [-9.6165e-02,  9.8663e-01, -6.3433e-01,  ...,  9.3425e-01,\n",
      "          -6.9079e-01, -8.1099e-01],\n",
      "         ...,\n",
      "         [ 1.7211e+00,  1.7923e+00,  1.1009e+00,  ...,  6.5221e-01,\n",
      "          -1.1670e+00, -6.0154e-01],\n",
      "         [ 8.2344e-01, -7.7899e-02, -1.1986e-01,  ...,  1.0274e+00,\n",
      "          -1.6017e+00, -5.1394e-01],\n",
      "         [ 3.5316e-03, -5.6932e-01,  4.3654e-01,  ...,  9.0265e-01,\n",
      "          -2.2796e-01,  9.0284e-01]]], grad_fn=<UnsafeViewBackward0>), shape is torch.Size([12, 32, 1024])\n",
      "loss is 7.329715728759766, loss.shape is torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=meta_vocab_size, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "\n",
    "\n",
    "\n",
    "logits, loss = model(X, Y)\n",
    "print(f'logits is {logits}, shape is {logits.shape}')\n",
    "print(f'loss is {loss}, loss.shape is {loss.shape}')\n",
    "# print(f'logits={logits}')\n",
    "\n",
    "# Y.shape\n",
    "# logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 7.3203, val loss 7.3350\n",
      "iter 0: loss 7.3841, time 82255.06ms, mfu -100.00%\n",
      "iter 1: loss 7.3503, time 8232.30ms, mfu -100.00%\n",
      "iter 2: loss 7.2621, time 8004.26ms, mfu -100.00%\n",
      "iter 3: loss 7.1068, time 8474.96ms, mfu -100.00%\n",
      "iter 4: loss 7.0411, time 8769.57ms, mfu -100.00%\n",
      "iter 5: loss 6.9560, time 9266.05ms, mfu 0.03%\n",
      "iter 6: loss 6.9347, time 8372.15ms, mfu 0.03%\n",
      "iter 7: loss 7.0125, time 9175.79ms, mfu 0.03%\n",
      "iter 8: loss 6.7482, time 9168.79ms, mfu 0.03%\n",
      "iter 9: loss 6.7615, time 6976.81ms, mfu 0.03%\n",
      "iter 10: loss 6.6667, time 6236.87ms, mfu 0.04%\n",
      "iter 11: loss 6.5987, time 6640.62ms, mfu 0.04%\n",
      "iter 12: loss 6.5105, time 9155.96ms, mfu 0.04%\n",
      "iter 13: loss 6.3522, time 9240.67ms, mfu 0.04%\n",
      "iter 14: loss 6.3059, time 8658.09ms, mfu 0.04%\n",
      "iter 15: loss 6.4829, time 7034.91ms, mfu 0.04%\n",
      "iter 16: loss 6.3115, time 6751.88ms, mfu 0.04%\n",
      "iter 17: loss 6.2224, time 6993.81ms, mfu 0.04%\n",
      "iter 18: loss 6.1203, time 8319.99ms, mfu 0.04%\n",
      "iter 19: loss 5.9731, time 8586.31ms, mfu 0.04%\n",
      "iter 20: loss 5.9364, time 6653.99ms, mfu 0.04%\n",
      "iter 21: loss 6.1714, time 6675.63ms, mfu 0.04%\n",
      "iter 22: loss 6.1066, time 7419.19ms, mfu 0.04%\n",
      "iter 23: loss 6.0149, time 6745.47ms, mfu 0.04%\n",
      "iter 24: loss 5.7899, time 8320.45ms, mfu 0.04%\n",
      "iter 25: loss 5.9951, time 6677.06ms, mfu 0.04%\n",
      "iter 26: loss 5.8517, time 6771.18ms, mfu 0.04%\n",
      "iter 27: loss 5.9782, time 6772.77ms, mfu 0.04%\n",
      "iter 28: loss 5.9164, time 6823.15ms, mfu 0.04%\n",
      "iter 29: loss 5.9063, time 6503.47ms, mfu 0.04%\n",
      "iter 30: loss 5.6662, time 8368.58ms, mfu 0.04%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[39m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     52\u001b[0m \u001b[39m# clip the gradient\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m grad_clip \u001b[39m!=\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nanogpt/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nanogpt/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
